---
title: "Other Observability Solutions"
description: "Deploy Prometheus, Grafana, Elastic, and other monitoring tools via Helm"
---

## Overview

Beyond Qovery Observe, Datadog, and New Relic, you can deploy any observability tool that provides an official Helm chart. This page covers popular open-source and commercial solutions deployable on Qovery.

## Popular Solutions

<CardGroup cols={2}>
  <Card title="Prometheus + Grafana" icon="chart-line">
    Industry-standard metrics and visualization stack
  </Card>

  <Card title="Elastic Stack (ELK)" icon="magnifying-glass">
    Powerful log aggregation and search platform
  </Card>

  <Card title="Loki + Grafana" icon="database">
    Lightweight log aggregation inspired by Prometheus
  </Card>

  <Card title="Kubecost" icon="dollar-sign">
    Kubernetes cost monitoring and optimization
  </Card>
</CardGroup>

## Prometheus + Grafana

### Overview

The `kube-prometheus-stack` provides a complete monitoring solution with Prometheus, Grafana, and pre-configured dashboards for Kubernetes.

### Deployment

<Steps>
  <Step title="Create Helm Service">
    In Qovery Console:
    - Name: `prometheus-stack`
    - Repository: `https://prometheus-community.github.io/helm-charts`
    - Chart: `kube-prometheus-stack`
    - Version: `55.0.0` (or latest)
  </Step>

  <Step title="Configure values.yaml">
    ```yaml
    # Prometheus configuration
    prometheus:
      enabled: true

      prometheusSpec:
        # Retention period
        retention: 30d

        # Storage
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi

        # Resource limits
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi

        # Scrape interval
        scrapeInterval: 30s

    # Grafana configuration
    grafana:
      enabled: true

      # Admin credentials
      adminPassword: changeme123!

      # Persistence
      persistence:
        enabled: true
        size: 10Gi

      # Ingress (to access Grafana UI)
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: nginx
        hosts:
          - grafana.your-domain.com

      # Pre-installed dashboards
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default

    # Alert Manager
    alertmanager:
      enabled: true

      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 5Gi

      # Alert configuration
      config:
        global:
          resolve_timeout: 5m

        route:
          group_by: ['alertname', 'cluster', 'service']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 12h
          receiver: 'slack'

        receivers:
          - name: 'slack'
            slack_configs:
              - api_url: 'YOUR_SLACK_WEBHOOK_URL'
                channel: '#alerts'
                title: 'Alert: {{ .GroupLabels.alertname }}'

    # Node Exporter (collects node metrics)
    nodeExporter:
      enabled: true

    # Kube State Metrics
    kubeStateMetrics:
      enabled: true
    ```
  </Step>

  <Step title="Deploy">
    Click **Deploy** and wait 3-5 minutes
  </Step>

  <Step title="Access Grafana">
    1. Navigate to `https://grafana.your-domain.com`
    2. Login with `admin` / your password
    3. Explore pre-built Kubernetes dashboards
  </Step>
</Steps>

### Key Features

- **Prometheus**: Time-series metrics database
- **Grafana**: Visualization and dashboards
- **AlertManager**: Alert routing and silencing
- **Pre-built dashboards**: 30+ Kubernetes dashboards included
- **ServiceMonitor CRDs**: Automatic service discovery

### Use Cases

✅ **Best for**: Teams wanting full control, open-source preference, custom dashboards
❌ **Not ideal for**: Teams wanting managed solution, APM tracing (use Tempo for that)

## Elastic Stack (ELK)

### Overview

Deploy Elasticsearch, Logstash, and Kibana for powerful log aggregation, search, and visualization.

### Deployment

<Steps>
  <Step title="Deploy Elasticsearch">
    **Helm Service 1**: Elasticsearch
    - Repository: `https://helm.elastic.co`
    - Chart: `elasticsearch`
    - Version: `8.5.1`

    ```yaml
    # values.yaml
    replicas: 3
    minimumMasterNodes: 2

    resources:
      requests:
        cpu: 1000m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi

    volumeClaimTemplate:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi

    # Security (disable for dev, enable for prod)
    esConfig:
      elasticsearch.yml: |
        xpack.security.enabled: false
    ```
  </Step>

  <Step title="Deploy Kibana">
    **Helm Service 2**: Kibana
    - Repository: `https://helm.elastic.co`
    - Chart: `kibana`
    - Version: `8.5.1`

    ```yaml
    # values.yaml
    elasticsearchHosts: "http://elasticsearch-master:9200"

    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi

    ingress:
      enabled: true
      hosts:
        - host: kibana.your-domain.com
          paths:
            - path: /
    ```
  </Step>

  <Step title="Deploy Filebeat (Log Shipper)">
    **Helm Service 3**: Filebeat
    - Repository: `https://helm.elastic.co`
    - Chart: `filebeat`
    - Version: `8.5.1`

    ```yaml
    # values.yaml
    daemonset:
      enabled: true

    filebeatConfig:
      filebeat.yml: |
        filebeat.inputs:
        - type: container
          paths:
            - /var/log/containers/*.log
          processors:
          - add_kubernetes_metadata:
              host: ${NODE_NAME}
              matchers:
              - logs_path:
                  logs_path: "/var/log/containers/"

        output.elasticsearch:
          hosts: ["elasticsearch-master:9200"]
          indices:
            - index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 500m
        memory: 500Mi
    ```
  </Step>

  <Step title="Access Kibana">
    Navigate to `https://kibana.your-domain.com` and create index patterns
  </Step>
</Steps>

### Key Features

- **Elasticsearch**: Distributed search and analytics
- **Kibana**: Visualization and exploration
- **Filebeat**: Lightweight log shipper
- **Powerful search**: Full-text search with complex queries
- **Machine learning**: Anomaly detection (paid feature)

### Use Cases

✅ **Best for**: Teams with heavy logging needs, complex search requirements
❌ **Not ideal for**: Small teams (complex setup), cost-sensitive (resource-intensive)

## Loki + Grafana

### Overview

Grafana Loki is a log aggregation system designed to be cost-effective and easy to operate, using labels like Prometheus.

### Deployment

<Steps>
  <Step title="Create Helm Service">
    - Name: `loki-stack`
    - Repository: `https://grafana.github.io/helm-charts`
    - Chart: `loki-stack`
    - Version: `2.9.0`
  </Step>

  <Step title="Configure values.yaml">
    ```yaml
    loki:
      enabled: true

      persistence:
        enabled: true
        size: 50Gi

      config:
        auth_enabled: false

        ingester:
          chunk_idle_period: 3m
          chunk_block_size: 262144
          chunk_retain_period: 1m

        limits_config:
          retention_period: 744h  # 31 days

        storage_config:
          boltdb_shipper:
            active_index_directory: /data/loki/boltdb-shipper-active
            cache_location: /data/loki/boltdb-shipper-cache
            shared_store: filesystem

          filesystem:
            directory: /data/loki/chunks

      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 1Gi

    # Promtail (log collector)
    promtail:
      enabled: true

      config:
        clients:
          - url: http://loki:3100/loki/api/v1/push

      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

    # Grafana
    grafana:
      enabled: true

      adminPassword: changeme123!

      persistence:
        enabled: true
        size: 10Gi

      ingress:
        enabled: true
        hosts:
          - grafana.your-domain.com

      # Pre-configure Loki datasource
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Loki
              type: loki
              access: proxy
              url: http://loki:3100
              isDefault: true
    ```
  </Step>

  <Step title="Deploy and Access">
    Deploy and access Grafana at your configured domain
  </Step>
</Steps>

### Key Features

- **Label-based indexing**: Like Prometheus for logs
- **Cost-effective**: Less storage than Elasticsearch
- **LogQL**: Powerful query language
- **Grafana integration**: Seamless integration
- **Low maintenance**: Simpler than ELK

### Use Cases

✅ **Best for**: Teams already using Prometheus/Grafana, cost-conscious
❌ **Not ideal for**: Complex log analysis, full-text search across all fields

## Kubecost

### Overview

Monitor and optimize Kubernetes costs with real-time cost allocation and insights.

### Deployment

<Steps>
  <Step title="Create Helm Service">
    - Name: `kubecost`
    - Repository: `https://kubecost.github.io/cost-analyzer/`
    - Chart: `cost-analyzer`
    - Version: `1.107.0`
  </Step>

  <Step title="Configure values.yaml">
    ```yaml
    kubecostProductConfigs:
      # Your cluster name
      clusterName: qovery-production

      # Cloud provider
      cloudProvider: AWS  # or GCP, Azure

    # Prometheus (can use existing Prometheus)
    prometheus:
      server:
        enabled: true
        retention: 30d

    # Persistent volume for data
    persistentVolume:
      enabled: true
      size: 32Gi

    # Ingress
    ingress:
      enabled: true
      hosts:
        - kubecost.your-domain.com

    # Resource requests
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi

    # Network costs (optional)
    networkCosts:
      enabled: true
    ```
  </Step>

  <Step title="Access Dashboard">
    Navigate to `https://kubecost.your-domain.com`
  </Step>
</Steps>

### Key Features

- **Cost allocation**: Per-namespace, deployment, pod, label
- **Recommendations**: Right-sizing, spot instances
- **Showback/Chargeback**: Team cost reports
- **Alerts**: Budget alerts
- **Multi-cluster**: Aggregate costs across clusters (paid)

### Use Cases

✅ **Best for**: FinOps teams, cost optimization, chargeback/showback
✅ **Free tier**: Full features for single cluster

## Other Solutions

### Dynatrace

**Commercial APM with AI**:
- Auto-discovery and instrumentation
- AI-powered root cause analysis
- Full-stack monitoring
- Deploy via `dynatrace-operator` Helm chart

**Helm**:
- Repository: `https://raw.githubusercontent.com/Dynatrace/helm-charts/master/repos/stable`
- Chart: `dynatrace-operator`

### Jaeger

**Distributed Tracing**:
- Open-source tracing platform
- Compatible with OpenTelemetry
- Service dependency analysis

**Helm**:
- Repository: `https://jaegertracing.github.io/helm-charts`
- Chart: `jaeger`

### Thanos

**Prometheus Long-term Storage**:
- Extends Prometheus with long-term storage
- Multi-cluster aggregation
- S3-compatible storage backend

**Helm**:
- Repository: `https://charts.bitnami.com/bitnami`
- Chart: `thanos`

### Victoria Metrics

**Prometheus Alternative**:
- Faster and more cost-effective than Prometheus
- Drop-in Prometheus replacement
- Better long-term storage

**Helm**:
- Repository: `https://victoriametrics.github.io/helm-charts/`
- Chart: `victoria-metrics-k8s-stack`

### Splunk

**Enterprise Log Management**:
- Enterprise logging and SIEM
- Advanced analytics and machine learning
- Compliance and security monitoring

**Helm**:
- Repository: `https://splunk.github.io/splunk-connect-for-kubernetes/`
- Chart: `splunk-kubernetes-logging`

## Choosing the Right Tool

<Tabs>
  <Tab title="By Use Case">
    **Quick Setup, Zero Config**:
    → Qovery Observe

    **Advanced APM, Managed**:
    → Datadog or New Relic

    **Full Control, Open Source**:
    → Prometheus + Grafana

    **Heavy Logging, Complex Search**:
    → Elastic Stack

    **Cost-Effective Logging**:
    → Loki + Grafana

    **Cost Monitoring**:
    → Kubecost

    **Distributed Tracing**:
    → Jaeger or Tempo

    **Long-term Metrics Storage**:
    → Thanos or Victoria Metrics
  </Tab>

  <Tab title="By Team Size">
    **1-5 developers**:
    → Qovery Observe (built-in)

    **5-20 developers**:
    → Qovery Observe + Prometheus/Grafana

    **20-50 developers**:
    → Datadog or New Relic (ease of use)

    **50+ developers**:
    → Datadog/New Relic or custom stack with multiple tools
  </Tab>

  <Tab title="By Budget">
    **$0/month**:
    → Qovery Observe, Prometheus, Grafana, Loki

    **$100-500/month**:
    → Qovery Observe + Kubecost
    → New Relic (free tier + paid)

    **$500-2000/month**:
    → Datadog (small deployment)
    → New Relic (medium deployment)

    **$2000+/month**:
    → Datadog (large deployment)
    → Dynatrace
    → Enterprise Splunk
  </Tab>
</Tabs>

## Deployment Tips

<CardGroup cols={2}>
  <Card title="Start Small" icon="seedling">
    Begin with Qovery Observe, add external tools as needed
  </Card>

  <Card title="Use Namespaces" icon="folder-tree">
    Deploy monitoring tools in dedicated namespace (e.g., `monitoring`)
  </Card>

  <Card title="Set Resource Limits" icon="gauge">
    Always set CPU/memory limits to prevent resource starvation
  </Card>

  <Card title="Configure Persistence" icon="database">
    Enable persistent volumes for data retention
  </Card>

  <Card title="Secure Access" icon="lock">
    Use strong passwords, enable HTTPS, restrict access
  </Card>

  <Card title="Monitor the Monitors" icon="eyes">
    Monitor your monitoring tools' resource usage
  </Card>
</CardGroup>

## Cost Comparison

| Solution | Setup Cost | Monthly Cost (10 nodes) | Data Retention | Complexity |
|----------|------------|-------------------------|----------------|------------|
| **Qovery Observe** | $0 | Included | 90 days | Low |
| **Prometheus + Grafana** | $0 | ~$50 (storage) | Configurable | Medium |
| **Loki + Grafana** | $0 | ~$30 (storage) | Configurable | Medium |
| **Elastic Stack** | $0 | ~$200 (storage + compute) | Configurable | High |
| **Datadog** | $0 | ~$800-1500 | 15 days default | Low |
| **New Relic** | $0 | ~$600-1200 | Based on plan | Low |
| **Kubecost** | $0 | Free (single cluster) | 15 days | Low |

*Costs are estimates and vary based on usage*

## Next Steps

<CardGroup cols={2}>
  <Card title="Qovery Observe" icon="chart-mixed" href="/integrations/observability/qovery-observe">
    Start with built-in observability
  </Card>

  <Card title="Datadog" icon="datadog" href="/integrations/observability/datadog">
    Deploy managed monitoring solution
  </Card>

  <Card title="New Relic" icon="chart-line" href="/integrations/observability/new-relic">
    Alternative managed solution
  </Card>

  <Card title="Helm Charts" icon="helm" href="/integrations/helm/overview">
    Learn more about deploying Helm charts
  </Card>
</CardGroup>

## Resources

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Elastic Documentation](https://www.elastic.co/guide/)
- [Loki Documentation](https://grafana.com/docs/loki/)
- [Kubecost Documentation](https://docs.kubecost.com/)
- [CNCF Observability Landscape](https://landscape.cncf.io/card-mode?category=observability-and-analysis)
