---
title: "AWS S3"
description: "Deploy and manage AWS S3 buckets using Terraform with Qovery as the orchestrator"
---

## Overview

Deploy AWS S3 (Simple Storage Service) buckets using Terraform, orchestrated by Qovery through Kubernetes. Provision object storage for your applications automatically alongside your containers, managing everything from a unified platform.

<Info>
**Universal Orchestration**: Qovery leverages Kubernetes to orchestrate not just containers, but any cloud resource. Deploy S3 buckets, configure permissions, and manage lifecycle policies using the same workflow as your applications.
</Info>

## Use Cases

<CardGroup cols={2}>
  <Card title="Static Assets" icon="image">
    Host website assets, images, and media files
  </Card>

  <Card title="File Uploads" icon="cloud-arrow-up">
    Store user-uploaded content securely
  </Card>

  <Card title="Backups" icon="floppy-disk">
    Automated application and database backups
  </Card>

  <Card title="Data Lakes" icon="database">
    Store large datasets for analytics
  </Card>

  <Card title="Logs Archive" icon="file-lines">
    Long-term log storage and compliance
  </Card>

  <Card title="CDN Origin" icon="globe">
    Origin server for CloudFront distribution
  </Card>
</CardGroup>

## Quick Start

### Basic S3 Bucket with Terraform

<Steps>
  <Step title="Create Terraform Configuration">
    ```hcl
    # s3.tf
    provider "aws" {
      region = var.aws_region
    }

    resource "aws_s3_bucket" "app_storage" {
      bucket = "${var.environment}-${var.app_name}-storage"

      tags = {
        Name        = "${var.environment}-${var.app_name}-storage"
        Environment = var.environment
        ManagedBy   = "Qovery"
      }
    }

    # Enable versioning
    resource "aws_s3_bucket_versioning" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      versioning_configuration {
        status = "Enabled"
      }
    }

    # Enable encryption
    resource "aws_s3_bucket_server_side_encryption_configuration" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      rule {
        apply_server_side_encryption_by_default {
          sse_algorithm = "AES256"
        }
      }
    }

    # Block public access
    resource "aws_s3_bucket_public_access_block" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      block_public_acls       = true
      block_public_policy     = true
      ignore_public_acls      = true
      restrict_public_buckets = true
    }

    # Lifecycle policy
    resource "aws_s3_bucket_lifecycle_configuration" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      rule {
        id     = "transition-old-versions"
        status = "Enabled"

        noncurrent_version_transition {
          noncurrent_days = 30
          storage_class   = "STANDARD_IA"
        }

        noncurrent_version_transition {
          noncurrent_days = 90
          storage_class   = "GLACIER"
        }

        noncurrent_version_expiration {
          noncurrent_days = 365
        }
      }
    }

    # IAM policy for application access
    resource "aws_iam_policy" "s3_access" {
      name        = "${var.environment}-${var.app_name}-s3-access"
      description = "Policy for application to access S3 bucket"

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [
          {
            Effect = "Allow"
            Action = [
              "s3:GetObject",
              "s3:PutObject",
              "s3:DeleteObject",
              "s3:ListBucket"
            ]
            Resource = [
              aws_s3_bucket.app_storage.arn,
              "${aws_s3_bucket.app_storage.arn}/*"
            ]
          }
        ]
      })
    }

    output "bucket_name" {
      value       = aws_s3_bucket.app_storage.id
      description = "S3 bucket name"
    }

    output "bucket_arn" {
      value       = aws_s3_bucket.app_storage.arn
      description = "S3 bucket ARN"
    }

    output "bucket_region" {
      value       = aws_s3_bucket.app_storage.region
      description = "S3 bucket region"
    }
    ```
  </Step>

  <Step title="Deploy via Lifecycle Job">
    ```yaml
    # Qovery Configuration
    version: "1.0"
    lifecycle_jobs:
      - name: provision-s3
        image: hashicorp/terraform:latest
        on:
          - ENVIRONMENT_START
        commands:
          - /bin/sh
          - -c
          - |
            cd /workspace/terraform/s3
            terraform init
            terraform apply -auto-approve \
              -var="environment=${QOVERY_ENVIRONMENT_NAME}" \
              -var="app_name=${APP_NAME}" \
              -var="aws_region=${AWS_REGION}"

            # Export outputs for applications
            terraform output -raw bucket_name > /qovery/bucket_name.txt
            terraform output -raw bucket_arn > /qovery/bucket_arn.txt
        environment_variables:
          - name: AWS_ACCESS_KEY_ID
            secret: true
          - name: AWS_SECRET_ACCESS_KEY
            secret: true
          - name: AWS_REGION
            value: us-east-1
          - name: APP_NAME
            value: myapp
          - name: TF_STATE_BUCKET
            value: my-terraform-state
    ```
  </Step>

  <Step title="Connect Application">
    ```yaml
    applications:
      - name: web-app
        environment_variables:
          - name: AWS_S3_BUCKET
            value: ${QOVERY_ENVIRONMENT_NAME}-myapp-storage
          - name: AWS_REGION
            value: us-east-1
          - name: AWS_ACCESS_KEY_ID
            secret: true
          - name: AWS_SECRET_ACCESS_KEY
            secret: true
    ```

    **Application Code**:
    ```javascript
    // Node.js example
    const AWS = require('aws-sdk');

    const s3 = new AWS.S3({
      region: process.env.AWS_REGION,
      accessKeyId: process.env.AWS_ACCESS_KEY_ID,
      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
    });

    const bucketName = process.env.AWS_S3_BUCKET;

    // Upload file
    async function uploadFile(key, body) {
      const params = {
        Bucket: bucketName,
        Key: key,
        Body: body
      };

      return await s3.upload(params).promise();
    }

    // Download file
    async function downloadFile(key) {
      const params = {
        Bucket: bucketName,
        Key: key
      };

      return await s3.getObject(params).promise();
    }
    ```
  </Step>
</Steps>

## Advanced Configurations

### Static Website Hosting

<Tabs>
  <Tab title="Terraform Config">
    ```hcl
    # static-website.tf
    resource "aws_s3_bucket" "website" {
      bucket = "${var.domain_name}"
    }

    resource "aws_s3_bucket_website_configuration" "website" {
      bucket = aws_s3_bucket.website.id

      index_document {
        suffix = "index.html"
      }

      error_document {
        key = "error.html"
      }
    }

    resource "aws_s3_bucket_policy" "website" {
      bucket = aws_s3_bucket.website.id

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [
          {
            Sid       = "PublicReadGetObject"
            Effect    = "Allow"
            Principal = "*"
            Action    = "s3:GetObject"
            Resource  = "${aws_s3_bucket.website.arn}/*"
          }
        ]
      })
    }

    # CloudFront distribution for HTTPS
    resource "aws_cloudfront_distribution" "website" {
      enabled             = true
      default_root_object = "index.html"

      origin {
        domain_name = aws_s3_bucket_website_configuration.website.website_endpoint
        origin_id   = "S3-${aws_s3_bucket.website.id}"

        custom_origin_config {
          http_port              = 80
          https_port             = 443
          origin_protocol_policy = "http-only"
          origin_ssl_protocols   = ["TLSv1.2"]
        }
      }

      default_cache_behavior {
        allowed_methods        = ["GET", "HEAD"]
        cached_methods         = ["GET", "HEAD"]
        target_origin_id       = "S3-${aws_s3_bucket.website.id}"
        viewer_protocol_policy = "redirect-to-https"

        forwarded_values {
          query_string = false
          cookies {
            forward = "none"
          }
        }

        min_ttl     = 0
        default_ttl = 3600
        max_ttl     = 86400
      }

      restrictions {
        geo_restriction {
          restriction_type = "none"
        }
      }

      viewer_certificate {
        cloudfront_default_certificate = true
      }
    }

    output "website_url" {
      value = aws_s3_bucket_website_configuration.website.website_endpoint
    }

    output "cloudfront_url" {
      value = aws_cloudfront_distribution.website.domain_name
    }
    ```
  </Tab>

  <Tab title="Deploy Script">
    ```yaml
    lifecycle_jobs:
      - name: deploy-website
        image: hashicorp/terraform:latest
        on:
          - ENVIRONMENT_START
        commands:
          - /bin/sh
          - -c
          - |
            # Provision S3 and CloudFront
            cd /workspace/terraform
            terraform init
            terraform apply -auto-approve

            # Sync website files
            aws s3 sync /workspace/build s3://${BUCKET_NAME}/ --delete

            # Invalidate CloudFront cache
            aws cloudfront create-invalidation \
              --distribution-id ${CLOUDFRONT_ID} \
              --paths "/*"
    ```
  </Tab>
</Tabs>

### Data Lake with Partitioning

<Tabs>
  <Tab title="Terraform Config">
    ```hcl
    # data-lake.tf
    resource "aws_s3_bucket" "data_lake" {
      bucket = "${var.environment}-data-lake"
    }

    resource "aws_s3_bucket_intelligent_tiering_configuration" "data_lake" {
      bucket = aws_s3_bucket.data_lake.id
      name   = "EntireBucket"

      tiering {
        access_tier = "DEEP_ARCHIVE_ACCESS"
        days        = 180
      }

      tiering {
        access_tier = "ARCHIVE_ACCESS"
        days        = 90
      }
    }

    # Glue database for querying
    resource "aws_glue_catalog_database" "data_lake" {
      name = "${var.environment}_data_lake"
    }

    # Athena workgroup
    resource "aws_athena_workgroup" "data_lake" {
      name = "${var.environment}-data-lake"

      configuration {
        result_configuration {
          output_location = "s3://${aws_s3_bucket.data_lake.id}/athena-results/"

          encryption_configuration {
            encryption_option = "SSE_S3"
          }
        }
      }
    }
    ```
  </Tab>

  <Tab title="Partition Structure">
    ```
    s3://production-data-lake/
    ├── events/
    │   ├── year=2024/
    │   │   ├── month=01/
    │   │   │   ├── day=01/
    │   │   │   │   └── data.parquet
    │   │   │   ├── day=02/
    │   │   │   └── day=03/
    │   │   └── month=02/
    │   └── year=2023/
    ├── users/
    │   └── snapshot-date=2024-01-15/
    │       └── users.parquet
    └── logs/
        ├── application=web-app/
        │   └── date=2024-01-15/
        │       └── logs.json.gz
        └── application=api/
    ```
  </Tab>
</Tabs>

### Backup Bucket

<Tabs>
  <Tab title="Configuration">
    ```hcl
    # backup-bucket.tf
    resource "aws_s3_bucket" "backups" {
      bucket = "${var.environment}-backups"
    }

    resource "aws_s3_bucket_versioning" "backups" {
      bucket = aws_s3_bucket.backups.id

      versioning_configuration {
        status = "Enabled"
        mfa_delete = "Enabled"  # Requires MFA to delete
      }
    }

    # Lifecycle policy for cost optimization
    resource "aws_s3_bucket_lifecycle_configuration" "backups" {
      bucket = aws_s3_bucket.backups.id

      rule {
        id     = "daily-backups"
        status = "Enabled"

        filter {
          prefix = "daily/"
        }

        transition {
          days          = 7
          storage_class = "STANDARD_IA"
        }

        transition {
          days          = 30
          storage_class = "GLACIER_IR"
        }

        transition {
          days          = 90
          storage_class = "DEEP_ARCHIVE"
        }

        expiration {
          days = 365
        }
      }

      rule {
        id     = "weekly-backups"
        status = "Enabled"

        filter {
          prefix = "weekly/"
        }

        transition {
          days          = 30
          storage_class = "GLACIER"
        }

        expiration {
          days = 730  # 2 years
        }
      }
    }

    # Replication to another region for DR
    resource "aws_s3_bucket_replication_configuration" "backups" {
      bucket = aws_s3_bucket.backups.id
      role   = aws_iam_role.replication.arn

      rule {
        id     = "replicate-all"
        status = "Enabled"

        destination {
          bucket        = aws_s3_bucket.backups_replica.arn
          storage_class = "GLACIER"

          replication_time {
            status = "Enabled"
            time {
              minutes = 15
            }
          }

          metrics {
            status = "Enabled"
            event_threshold {
              minutes = 15
            }
          }
        }
      }
    }
    ```
  </Tab>

  <Tab title="Backup Script">
    ```yaml
    cronjobs:
      - name: database-backup
        schedule: "0 2 * * *"  # Daily at 2 AM
        image: postgres:15
        commands:
          - /bin/sh
          - -c
          - |
            DATE=$(date +%Y-%m-%d-%H%M%S)
            FILENAME="daily/postgres-${DATE}.sql.gz"

            # Dump and compress
            pg_dump ${DATABASE_URL} | gzip > /tmp/backup.sql.gz

            # Upload to S3
            aws s3 cp /tmp/backup.sql.gz s3://${BACKUP_BUCKET}/${FILENAME}

            echo "Backup completed: ${FILENAME}"
        environment_variables:
          - name: DATABASE_URL
            secret: true
          - name: BACKUP_BUCKET
            value: production-backups
          - name: AWS_ACCESS_KEY_ID
            secret: true
          - name: AWS_SECRET_ACCESS_KEY
            secret: true
    ```
  </Tab>
</Tabs>

## Security Best Practices

<AccordionGroup>
  <Accordion title="Access Control" icon="lock">
    **Principle of Least Privilege**:
    ```hcl
    # Separate policies for different access levels
    resource "aws_iam_policy" "read_only" {
      name = "s3-read-only"

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Action = [
            "s3:GetObject",
            "s3:ListBucket"
          ]
          Resource = [
            aws_s3_bucket.app_storage.arn,
            "${aws_s3_bucket.app_storage.arn}/*"
          ]
        }]
      })
    }

    resource "aws_iam_policy" "full_access" {
      name = "s3-full-access"

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Action = [
            "s3:GetObject",
            "s3:PutObject",
            "s3:DeleteObject",
            "s3:ListBucket"
          ]
          Resource = [
            aws_s3_bucket.app_storage.arn,
            "${aws_s3_bucket.app_storage.arn}/*"
          ]
        }]
      })
    }
    ```

    **Use IAM Roles for Service Accounts (IRSA)**:
    ```hcl
    # EKS pod can assume this role
    resource "aws_iam_role" "pod_s3_access" {
      name = "eks-pod-s3-access"

      assume_role_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Principal = {
            Federated = aws_iam_openid_connect_provider.eks.arn
          }
          Action = "sts:AssumeRoleWithWebIdentity"
          Condition = {
            StringEquals = {
              "${aws_iam_openid_connect_provider.eks.url}:sub": "system:serviceaccount:default:my-app"
            }
          }
        }]
      })
    }
    ```
  </Accordion>

  <Accordion title="Encryption" icon="shield">
    **Server-Side Encryption with KMS**:
    ```hcl
    resource "aws_kms_key" "s3" {
      description             = "KMS key for S3 encryption"
      deletion_window_in_days = 7
      enable_key_rotation     = true
    }

    resource "aws_s3_bucket_server_side_encryption_configuration" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      rule {
        apply_server_side_encryption_by_default {
          sse_algorithm     = "aws:kms"
          kms_master_key_id = aws_kms_key.s3.arn
        }
        bucket_key_enabled = true
      }
    }
    ```

    **Enforce HTTPS Only**:
    ```hcl
    resource "aws_s3_bucket_policy" "require_https" {
      bucket = aws_s3_bucket.app_storage.id

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Sid    = "DenyInsecureTransport"
          Effect = "Deny"
          Principal = "*"
          Action = "s3:*"
          Resource = [
            aws_s3_bucket.app_storage.arn,
            "${aws_s3_bucket.app_storage.arn}/*"
          ]
          Condition = {
            Bool = {
              "aws:SecureTransport" = "false"
            }
          }
        }]
      })
    }
    ```
  </Accordion>

  <Accordion title="Public Access Prevention" icon="ban">
    ```hcl
    # Always block public access unless specifically needed
    resource "aws_s3_bucket_public_access_block" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      block_public_acls       = true
      block_public_policy     = true
      ignore_public_acls      = true
      restrict_public_buckets = true
    }

    # Account-level public access block
    resource "aws_s3_account_public_access_block" "account" {
      block_public_acls       = true
      block_public_policy     = true
      ignore_public_acls      = true
      restrict_public_buckets = true
    }
    ```
  </Accordion>

  <Accordion title="Logging and Monitoring" icon="eye">
    ```hcl
    # Server access logging
    resource "aws_s3_bucket_logging" "app_storage" {
      bucket = aws_s3_bucket.app_storage.id

      target_bucket = aws_s3_bucket.logs.id
      target_prefix = "s3-access-logs/"
    }

    # CloudTrail for API calls
    resource "aws_cloudtrail" "s3_events" {
      name                          = "s3-data-events"
      s3_bucket_name               = aws_s3_bucket.cloudtrail.id
      include_global_service_events = false

      event_selector {
        read_write_type           = "All"
        include_management_events = true

        data_resource {
          type   = "AWS::S3::Object"
          values = ["${aws_s3_bucket.app_storage.arn}/*"]
        }
      }
    }
    ```
  </Accordion>
</AccordionGroup>

## Cost Optimization

<CardGroup cols={2}>
  <Card title="Storage Classes" icon="layer-group">
    Use appropriate storage class for access patterns
  </Card>

  <Card title="Lifecycle Policies" icon="clock-rotate-left">
    Automatically transition to cheaper storage
  </Card>

  <Card title="Intelligent Tiering" icon="wand-magic-sparkles">
    Automatic cost optimization based on access
  </Card>

  <Card title="S3 Analytics" icon="chart-line">
    Monitor access patterns to optimize
  </Card>
</CardGroup>

**Storage Class Selection**:
<Tabs>
  <Tab title="Frequently Accessed">
    - **S3 Standard**: Default, low latency
    - Use for: Active application data, website content
    - Cost: ~$0.023/GB/month
  </Tab>

  <Tab title="Infrequent Access">
    - **S3 Standard-IA**: Lower storage cost, retrieval fee
    - Use for: Backups, disaster recovery
    - Cost: ~$0.0125/GB/month + retrieval
  </Tab>

  <Tab title="Archive">
    - **S3 Glacier**: Very low cost, minutes to hours retrieval
    - **S3 Glacier Deep Archive**: Lowest cost, 12 hours retrieval
    - Use for: Long-term archives, compliance
    - Cost: ~$0.004/GB/month (Glacier), ~$0.00099/GB/month (Deep Archive)
  </Tab>

  <Tab title="Intelligent">
    - **S3 Intelligent-Tiering**: Automatic optimization
    - Use for: Unknown or changing access patterns
    - Cost: Variable + $0.0025/1000 objects monitoring fee
  </Tab>
</Tabs>

## Monitoring and Alerts

```hcl
# CloudWatch metrics
resource "aws_cloudwatch_metric_alarm" "bucket_size" {
  alarm_name          = "${var.environment}-s3-size"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "BucketSizeBytes"
  namespace           = "AWS/S3"
  period              = "86400"
  statistic           = "Average"
  threshold           = "107374182400"  # 100 GB
  alarm_description   = "S3 bucket size exceeded 100GB"

  dimensions = {
    BucketName = aws_s3_bucket.app_storage.id
    StorageType = "StandardStorage"
  }
}

resource "aws_cloudwatch_metric_alarm" "request_errors" {
  alarm_name          = "${var.environment}-s3-errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "4xxErrors"
  namespace           = "AWS/S3"
  period              = "300"
  statistic           = "Sum"
  threshold           = "10"
  alarm_description   = "High number of S3 4xx errors"

  dimensions = {
    BucketName = aws_s3_bucket.app_storage.id
  }
}
```

## Next Steps

<CardGroup cols={2}>
  <Card title="AWS RDS" icon="database" href="/integrations/terraform/aws-rds">
    Deploy RDS databases with Terraform
  </Card>

  <Card title="Cloudflare Worker" icon="bolt" href="/integrations/terraform/cloudflare-worker">
    Deploy edge functions with Qovery
  </Card>

  <Card title="Lifecycle Jobs" icon="rotate" href="/using-qovery/configuration/lifecycle-job">
    Learn about lifecycle jobs
  </Card>

  <Card title="Terraform Overview" icon="code" href="/integrations/terraform/overview">
    Terraform integration overview
  </Card>
</CardGroup>
