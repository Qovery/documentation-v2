---
title: "Clusters"
description: "Configure and manage Kubernetes clusters across multiple cloud providers"
---

## Overview

A cluster is a Kubernetes cluster managed by Qovery where your applications, databases, and services run. Qovery simplifies cluster management by handling provisioning, scaling, upgrades, and security across multiple cloud providers.

<Info>
Qovery supports both **managed clusters** (provisioned and managed by Qovery) and **BYOK** (Bring Your Own Kubernetes) clusters that you manage yourself.
</Info>

**Supported Cloud Providers**:
- [Amazon Web Services (AWS EKS)](/integrations/kubernetes/eks/overview)
- [Google Cloud Platform (GCP GKE)](/integrations/kubernetes/gke/overview)
- [Microsoft Azure (AKS)](/integrations/kubernetes/aks/overview)
- [Scaleway (Kapsule)](/integrations/kubernetes/scaleway/overview)
- [Bring Your Own Kubernetes (BYOK)](/integrations/kubernetes/byok)

## Creating a Cluster

<Info>
For detailed cluster creation instructions specific to your cloud provider, please refer to the integration guides:
- [AWS EKS Setup](/integrations/kubernetes/eks/managed)
- [GCP GKE Setup](/integrations/kubernetes/gke/managed)
- [Azure AKS Setup](/integrations/kubernetes/aks/managed)
- [Scaleway Kapsule Setup](/integrations/kubernetes/scaleway/managed)
- [Bring Your Own Kubernetes](/integrations/kubernetes/byok)
</Info>

<Steps>
  <Step title="Navigate to Clusters">
    In the Qovery Console, go to your **Organization** → **Clusters**

    <Frame>
      <img src="/images/configuration/clusters/cluster_section_access.png" alt="Access Clusters Section" />
    </Frame>
  </Step>

  <Step title="Add New Cluster">
    Click **Create Cluster** and select your cloud provider
  </Step>

  <Step title="Follow Provider-Specific Guide">
    Each cloud provider has unique requirements and configuration options. Follow the appropriate guide:

    <CardGroup cols={2}>
      <Card title="AWS EKS" icon="aws" href="/integrations/kubernetes/eks/managed">
        Create managed EKS cluster
      </Card>
      <Card title="GCP GKE" icon="google" href="/integrations/kubernetes/gke/managed">
        Create managed GKE cluster
      </Card>
      <Card title="Azure AKS" icon="microsoft" href="/integrations/kubernetes/aks/managed">
        Create managed AKS cluster
      </Card>
      <Card title="Scaleway" icon="server" href="/integrations/kubernetes/scaleway/managed">
        Create managed Kapsule cluster
      </Card>
    </CardGroup>
  </Step>
</Steps>

## Cluster Overview

Once your cluster is created, the overview page provides comprehensive information about cluster health, resources, and status.

<Frame>
  <img src="/images/configuration/clusters/cluster-overview.png" alt="Cluster Overview" />
</Frame>

### General Information

**Cluster Details**:
- **Name**: Unique cluster identifier
- **Cloud Provider**: AWS, GCP, Azure, or Scaleway
- **Region**: Geographic location
- **Kubernetes Version**: Current K8s version
- **Status**: Current cluster state
- **Created**: Cluster creation date

### Status Indicators

<Frame>
  <img src="/images/configuration/clusters/cluster_statuses.png" alt="Cluster Status Indicators" />
</Frame>

**Cluster States**:
- <Icon icon="circle-check" iconType="solid" color="green" /> **Running**: Cluster is healthy and operational
- <Icon icon="triangle-exclamation" iconType="solid" color="yellow" /> **Warning**: Minor issues detected, requires attention
- <Icon icon="circle-xmark" iconType="solid" color="red" /> **Error**: Critical issues, intervention required
- <Icon icon="circle" iconType="regular" color="gray" /> **Unavailable**: Cluster unreachable or offline

### Resource Information

View cluster capacity and utilization:

**Metrics Displayed**:
- **Nodes**: Total number of nodes and status
- **CPU**: Total vCPU capacity and current usage
- **Memory**: Total RAM capacity and current usage
- **Pods**: Running pods vs maximum capacity
- **Services**: Number of deployed services

**Example**:
```
Cluster: production-us-east-1
Nodes: 5/10 (auto-scaling enabled)
CPU: 24/40 vCPU (60% utilized)
Memory: 96/160 GB (60% utilized)
Pods: 47/110
Services: 15 applications, 3 databases
```

<Tip>
Monitor resource utilization regularly to optimize costs and ensure adequate capacity. Qovery provides recommendations when resources are over or under-utilized.
</Tip>

### Deployment Status

Track ongoing cluster operations and deployment history:

**Deployment States**:
- **Queued**: Operation waiting to execute
- **In Progress**: Changes being applied
- **Success**: Operation completed successfully
- **Failed**: Operation encountered errors

View detailed logs for each deployment to troubleshoot issues or verify successful operations.

## Cluster Settings

Configure cluster-wide settings and preferences.

### General Settings

**Editable Settings**:
- **Cluster Name**: Update cluster identifier
- **Description**: Add cluster purpose and details
- **Tags**: Organize clusters with custom tags

### Network Configuration

**Network Settings**:
- **VPC/Network**: View VPC/network information
- **CIDR Block**: IP address range for cluster
- **Subnets**: Public and private subnet configuration
- **Load Balancer**: Ingress load balancer settings

<Info>
For advanced networking configurations like VPC peering, see our [AWS VPC Peering Guide](/guides/others/aws-vpc-peering).
</Info>

### Features

**Available Features**:
- **Custom Domain**: Configure wildcard domains for services
- **Static IP**: Reserve static IP addresses
- **Private Registry**: Configure private container registries
- **Observability**: Enable monitoring and logging integrations

### Advanced Settings

Access cloud provider-specific advanced settings:
- Node instance types
- Auto-scaling configuration
- Storage classes
- Add-ons and plugins
- Security policies

<Warning>
Some advanced settings may impact cluster stability or cost. Review documentation before making changes.
</Warning>

## Cluster Actions

<Frame>
  <img src="/images/configuration/clusters/cluster_actions.png" alt="Cluster Actions" />
</Frame>

### Update Cluster

Modify cluster configuration without recreating it:

**Updatable Components**:
- Node instance types
- Number of nodes (min/max for auto-scaling)
- Kubernetes add-ons
- Network policies
- Security settings

<Steps>
  <Step title="Access Settings">
    Navigate to **Cluster** → **Settings**
  </Step>

  <Step title="Modify Configuration">
    Update desired settings (instance types, scaling, etc.)
  </Step>

  <Step title="Review Changes">
    Review the proposed changes and estimated impact
  </Step>

  <Step title="Apply Update">
    Click **Update** to apply changes with rolling updates
  </Step>
</Steps>

<Info>
Most updates are performed with zero downtime using rolling node replacements. Your services remain available during the update.
</Info>

### Stop Cluster

Temporarily stop a cluster to reduce costs:

<Steps>
  <Step title="Stop Cluster">
    Click **Actions** → **Stop Cluster**
  </Step>

  <Step title="Confirm">
    Confirm the action. All services will be stopped.
  </Step>
</Steps>

**What Happens**:
- All running services are stopped gracefully
- Nodes are terminated
- Persistent data remains intact
- Cloud costs reduced to storage only

**To Restart**: Use **Actions** → **Start Cluster** to restore the cluster and redeploy services.

<Tip>
Stop development and staging clusters during off-hours to significantly reduce infrastructure costs.
</Tip>

### Delete Cluster

Permanently remove a cluster and its resources:

<Warning>
Cluster deletion is **permanent and irreversible**. All services, data, and configurations will be lost unless backed up separately.
</Warning>

<Steps>
  <Step title="Prepare for Deletion">
    - Backup important data and configurations
    - Export Terraform state if using IaC
    - Document any custom configurations
    - Stop all services (recommended)
  </Step>

  <Step title="Initiate Deletion">
    Click **Actions** → **Delete Cluster**
  </Step>

  <Step title="Choose Deletion Mode">
    Select one of the following:

    **Option 1: Delete Everything** (Recommended)
    - Removes all Qovery-managed resources
    - Deletes cloud provider infrastructure
    - Complete cleanup with no residual costs

    **Option 2: Keep Cloud Resources**
    - Removes Qovery management only
    - Keeps cloud infrastructure running
    - Continue managing manually in cloud console
  </Step>

  <Step title="Confirm Deletion">
    Type the cluster name to confirm deletion
  </Step>
</Steps>

**Deletion Process**:
1. Stop all running services
2. Delete Kubernetes workloads
3. Terminate cluster nodes
4. Remove load balancers and networking
5. Delete persistent volumes (if selected)
6. Clean up cloud provider resources

<Tip>
Before deletion, export your configuration using Qovery Terraform Provider or CLI for potential future recreation.
</Tip>

### View Logs

Access cluster infrastructure logs for troubleshooting:

<Frame>
  <img src="/images/configuration/clusters/cluster_logs_access.png" alt="Access Cluster Logs" />
</Frame>

<Steps>
  <Step title="Access Logs">
    Click **Actions** → **View Logs**
  </Step>

  <Step title="Select Log Type">
    Choose log category:
    - **Infrastructure**: Cluster provisioning and management
    - **Kubernetes**: K8s control plane events
    - **Network**: Load balancer and ingress logs
  </Step>

  <Step title="Filter and Search">
    - Filter by time range
    - Search by keyword
    - Filter by severity level
  </Step>
</Steps>

**Successful Operations**:

<Frame>
  <img src="/images/configuration/clusters/ok-infra-logs.jpg" alt="Successful Cluster Logs" />
</Frame>

**Error Logs**:

<Frame>
  <img src="/images/configuration/clusters/error-infra-logs.jpg" alt="Cluster Error Logs" />
</Frame>

### Access Kubeconfig

Download Kubernetes configuration for direct cluster access:

<Steps>
  <Step title="Download Kubeconfig">
    Click **Actions** → **Download Kubeconfig**
  </Step>

  <Step title="Save Configuration">
    Save the kubeconfig file:
    ```bash
    # Default location
    mv ~/Downloads/kubeconfig ~/.kube/config

    # Or specify custom location
    export KUBECONFIG=~/path/to/kubeconfig
    ```
  </Step>

  <Step title="Verify Access">
    Test cluster access:
    ```bash
    kubectl cluster-info
    kubectl get nodes
    kubectl get pods --all-namespaces
    ```
  </Step>
</Steps>

<Warning>
**Security Best Practices**:
- Kubeconfig provides cluster-admin access
- Never commit kubeconfig to version control
- Store securely (use secret management tools)
- Rotate credentials regularly
- Use RBAC for team member access instead
</Warning>

<Info>
Direct kubectl access should be used sparingly. Qovery Console provides safer interfaces for most operations. Use kubectl primarily for debugging or advanced scenarios not covered by the Console.
</Info>

## Kubernetes Version Management

### Current Version

View your cluster's Kubernetes version in the cluster overview:

**Version Information**:
- **Current Version**: Currently running K8s version
- **Latest Available**: Newest supported version
- **End of Support**: When current version reaches EOL

### Upgrade Process

Qovery manages Kubernetes upgrades to ensure security and stability:

**Managed Upgrade Benefits**:
- Automated pre-upgrade health checks
- Rolling node upgrades with zero downtime
- Service health monitoring during upgrade
- Automatic rollback on failure
- Post-upgrade verification

<Steps>
  <Step title="Notification">
    Qovery notifies you when new versions are available
  </Step>

  <Step title="Review Release Notes">
    Check Kubernetes changelog and breaking changes
  </Step>

  <Step title="Test in Staging">
    Upgrade staging/development clusters first
  </Step>

  <Step title="Schedule Production Upgrade">
    Choose low-traffic window for production upgrades
  </Step>

  <Step title="Monitor Upgrade">
    Watch upgrade progress and service health
  </Step>
</Steps>

<Warning>
**Never upgrade Kubernetes manually** through your cloud provider console. This can cause incompatibilities with Qovery components and service disruptions. Always use Qovery-managed upgrades.
</Warning>

### Version Support Policy

**Qovery Support**:
- Latest 3 Kubernetes minor versions fully supported
- Security patches applied automatically
- 3-month notice before version deprecation
- Forced upgrades only after extended grace period

**Example Timeline**:
```
Version 1.28 Released → Fully Supported (12 months)
Version 1.29 Released → 1.28 still supported (9 months remaining)
Version 1.30 Released → 1.28 supported (6 months remaining)
Version 1.31 Released → 1.28 deprecated (upgrade recommended)
Version 1.32 Released → 1.28 end-of-support (forced upgrade)
```

## Security and Compliance

### Vulnerability Management

Qovery provides comprehensive security monitoring:

**Automated Security**:
- Continuous vulnerability scanning
- CVE database integration
- Real-time security alerts
- Severity classification (Critical, High, Medium, Low)

**What's Monitored**:
- Kubernetes control plane components
- Node operating system packages
- Container runtime
- Cluster add-ons and plugins

### Security Patching

When vulnerabilities are detected:

1. **Assessment**: Qovery evaluates impact and severity
2. **Patch Preparation**: Security patches are tested
3. **Notification**: Customers receive detailed advisories
4. **Guided Remediation**: Step-by-step instructions provided
5. **Automated Patching**: Critical issues auto-patched (optional)

### Compliance Features

**Security Capabilities**:
- Network policies and isolation
- RBAC and access controls
- Audit logging
- Secrets management
- Image scanning
- SOC 2 compliance

<Info>
Learn more about Qovery's security and compliance certifications in our [Security Overview](/security-and-compliance/overview).
</Info>

## Monitoring and Observability

### Built-in Metrics

Qovery provides cluster-level monitoring:

**Infrastructure Metrics**:
- Node CPU and memory utilization
- Disk I/O and network throughput
- Pod resource consumption
- Control plane health

**Accessing Metrics**:
- Navigate to **Cluster** → **Monitoring**
- Select time range (1h, 6h, 24h, 7d, 30d)
- Analyze trends and identify bottlenecks

### Alerting

Configure alerts for cluster issues:

**Alert Types**:
- High CPU usage (>80%)
- High memory usage (>85%)
- Node unavailable
- Disk space critical (&lt;20%)
- Control plane issues

**Notification Channels**:
- Email
- Slack
- Webhooks
- PagerDuty
- Custom integrations

### External Observability

Integrate with external monitoring solutions:

<CardGroup cols={2}>
  <Card title="Datadog" icon="chart-line" href="/integrations/observability/datadog">
    Full-stack monitoring and APM
  </Card>
  <Card title="New Relic" icon="chart-area" href="/integrations/observability/new-relic">
    Application performance monitoring
  </Card>
  <Card title="Qovery Observe" icon="magnifying-glass-chart" href="/integrations/observability/qovery-observe">
    Qovery's native observability platform
  </Card>
  <Card title="Other Solutions" icon="plug" href="/integrations/observability/other-solutions">
    Prometheus, Grafana, and more
  </Card>
</CardGroup>

## Multi-Cluster Management

Manage multiple clusters across regions and cloud providers:

### Common Patterns

**Geographic Distribution**:
```
Cluster 1: AWS us-east-1 (Production - North America)
Cluster 2: AWS eu-west-1 (Production - Europe)
Cluster 3: AWS ap-southeast-1 (Production - Asia Pacific)
```

**Environment Separation**:
```
Cluster 1: Production (isolated, high availability)
Cluster 2: Staging + QA
Cluster 3: Development + Preview Environments
```

**Multi-Cloud Strategy**:
```
Cluster 1: AWS EKS (Primary)
Cluster 2: GCP GKE (Disaster recovery)
Cluster 3: Azure AKS (Region-specific compliance)
```

### Best Practices

<AccordionGroup>
  <Accordion title="Naming Conventions">
    Use consistent, descriptive cluster names:
    ```
    Format: {environment}-{provider}-{region}
    Examples:
    - production-aws-us-east-1
    - staging-gcp-europe-west1
    - dev-scaleway-par1
    ```
  </Accordion>

  <Accordion title="Cluster Tagging">
    Tag clusters for organization and cost tracking:
    - **Environment**: production, staging, development
    - **Team**: backend, frontend, data
    - **Cost Center**: engineering, operations
    - **Purpose**: web-apps, ml-workloads, data-processing
  </Accordion>

  <Accordion title="Access Management">
    Control cluster access with RBAC:
    - Limit cluster-admin access
    - Use team-based permissions
    - Implement least-privilege access
    - Audit access regularly
  </Accordion>

  <Accordion title="Cross-Cluster Networking">
    For services spanning multiple clusters:
    - Use VPC/VNet peering
    - Implement service mesh (Istio, Linkerd)
    - Configure cross-cluster DNS
    - Set up global load balancing
  </Accordion>
</AccordionGroup>

## Best Practices

<CardGroup cols={2}>
  <Card title="Right-Size Your Cluster" icon="gauge-high">
    - Start with smaller instances
    - Enable auto-scaling for flexibility
    - Monitor utilization regularly
    - Leave 30% capacity buffer for traffic spikes
  </Card>

  <Card title="Kubernetes Versions" icon="code-branch">
    - Never manually upgrade from cloud console
    - Stay within supported version range
    - Upgrade non-production first
    - Review release notes before upgrading
  </Card>

  <Card title="High Availability" icon="shield-check">
    - Use minimum 3 nodes for production
    - Enable multi-AZ/zone deployment
    - Implement pod disruption budgets
    - Test failover scenarios regularly
  </Card>

  <Card title="Cost Optimization" icon="piggy-bank">
    - Right-size node instance types
    - Use spot/preemptible instances where safe
    - Stop dev/test clusters after hours
    - Review and optimize resource requests
  </Card>

  <Card title="Security" icon="lock">
    - Rotate kubeconfig credentials regularly
    - Limit cluster-admin access
    - Enable audit logging
    - Review security advisories promptly
  </Card>

  <Card title="Monitoring" icon="chart-mixed">
    - Set up critical alerts
    - Review cluster health weekly
    - Monitor costs and trends
    - Document baseline performance
  </Card>
</CardGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Cluster Shows Warning Status">
    **Symptoms**: Yellow warning indicator on cluster

    **Common Causes**:
    - One or more nodes unhealthy
    - High resource utilization (>80%)
    - Network connectivity issues
    - Recent failed deployments

    **Resolution Steps**:
    1. Check cluster overview for specific warnings
    2. Review node status: `kubectl get nodes`
    3. Check resource utilization in monitoring
    4. Review recent deployment logs
    5. Restart unhealthy nodes if needed
    6. Scale cluster if resources are constrained
  </Accordion>

  <Accordion title="Cluster Creation Failed">
    **Common Issues**:
    - Insufficient cloud provider quotas
    - Invalid or expired credentials
    - Region capacity limitations
    - Network configuration errors
    - Exceeded account limits

    **Resolution**:
    1. Check deployment logs for specific error message
    2. Verify cloud credentials are valid and have permissions
    3. Check quota limits in cloud provider console
    4. Try alternative region or instance type
    5. Contact Qovery support with cluster ID
  </Accordion>

  <Accordion title="Services Won't Deploy">
    **Check**:
    - Cluster status is Running (green)
    - Sufficient CPU/memory available
    - Image pull secrets configured
    - No network policy conflicts
    - Container registry accessible

    **Debug Steps**:
    ```bash
    # Check cluster nodes
    kubectl get nodes

    # Check pod events
    kubectl describe pod <pod-name> -n <namespace>

    # Check resource availability
    kubectl top nodes

    # Check container logs
    kubectl logs <pod-name> -n <namespace>
    ```
  </Accordion>

  <Accordion title="Cannot Access with kubectl">
    **Troubleshooting**:

    1. **Verify kubeconfig**:
       ```bash
       kubectl cluster-info
       ```

    2. **Check kubectl version**:
       ```bash
       kubectl version --client
       ```

    3. **Test connectivity**:
       ```bash
       kubectl get nodes
       ```

    4. **Common errors**:
       - `Unable to connect`: Check network/VPN connection
       - `Unauthorized`: Download fresh kubeconfig from Qovery
       - `Certificate expired`: Kubeconfig may need rotation

    5. **Download fresh kubeconfig** from Qovery Console
  </Accordion>

  <Accordion title="Cluster Status Unavailable">
    <Frame>
      <img src="/images/configuration/clusters/cluster_status_unavailable.png" alt="Cluster Unavailable Status" />
    </Frame>

    **Symptoms**: Gray/unavailable status indicator

    **Possible Causes**:
    - Cloud provider outage
    - Network connectivity lost
    - Control plane issues
    - Cluster manually modified outside Qovery

    **Resolution**:
    1. Check cloud provider status page
    2. Verify cloud credentials haven't expired
    3. Check Qovery status page
    4. Review cluster logs in Qovery Console
    5. Contact Qovery support if issue persists
  </Accordion>

  <Accordion title="High Cloud Costs">
    **Cost Optimization Checklist**:

    ✅ **Node Optimization**:
    - Review instance types (downsize if possible)
    - Enable auto-scaling to scale down during low usage
    - Use spot/preemptible instances for non-critical workloads

    ✅ **Resource Optimization**:
    - Review application resource requests/limits
    - Identify idle or underutilized services
    - Delete unused persistent volumes

    ✅ **Scheduling**:
    - Stop dev/test clusters outside business hours
    - Schedule non-critical batch jobs during off-peak

    ✅ **Monitoring**:
    - Enable cost allocation tags
    - Review cloud provider cost explorer
    - Set up budget alerts
  </Accordion>
</AccordionGroup>

## Cloud Provider Cleanup

If cluster deletion fails or leaves resources behind:

<Frame>
  <img src="/images/configuration/clusters/aws-console-cluster-cleanup.jpg" alt="Manual Cluster Cleanup in AWS Console" />
</Frame>

<Warning>
Manual cleanup should only be done if Qovery deletion fails. This can lead to orphaned resources or unexpected costs if not done carefully.
</Warning>

**Manual Cleanup Steps** (AWS Example):
1. Delete EKS cluster in AWS Console
2. Delete associated node groups
3. Delete VPC and subnets (if dedicated)
4. Delete load balancers
5. Delete security groups
6. Delete EBS volumes
7. Verify all resources deleted to avoid lingering costs

<Info>
For other cloud providers (GCP, Azure, Scaleway), refer to their respective console documentation for resource cleanup.
</Info>

## Next Steps

<CardGroup cols={2}>
  <Card title="Create Environment" icon="layer-group" href="/configuration/environment">
    Set up your first environment on the cluster
  </Card>
  <Card title="Deploy Application" icon="rocket" href="/guides/getting-started/deploy-your-first-application">
    Deploy your first application
  </Card>
  <Card title="AWS EKS Integration" icon="aws" href="/integrations/kubernetes/eks/overview">
    Learn more about AWS EKS clusters
  </Card>
  <Card title="Multi-Cluster Management" icon="server" href="/guides/use-cases/multi-kubernetes-clusters-management">
    Advanced multi-cluster strategies
  </Card>
</CardGroup>
