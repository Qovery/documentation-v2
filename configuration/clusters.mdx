---
title: "Clusters"
description: "Configure and manage Kubernetes clusters across multiple cloud providers"
---

## Overview

A cluster is a Kubernetes cluster managed by Qovery where your applications, databases, and services run. Qovery simplifies cluster management by handling provisioning, scaling, upgrades, and security across multiple cloud providers.

<Info>
Qovery supports both **managed clusters** (provisioned and managed by Qovery) and **BYOK** (Bring Your Own Kubernetes) clusters that you manage yourself.
</Info>

**Supported Cloud Providers**:
- [Amazon Web Services (AWS EKS)](/integrations/kubernetes/eks/overview)
- [Google Cloud Platform (GCP GKE)](/integrations/kubernetes/gke/overview)
- [Microsoft Azure (AKS)](/integrations/kubernetes/aks/overview)
- [Scaleway (Kapsule)](/integrations/kubernetes/scaleway/overview)
- [Bring Your Own Kubernetes (BYOK)](/integrations/kubernetes/byok)

## Creating a Cluster

<Info>
For detailed cluster creation instructions specific to your cloud provider, please refer to the integration guides:
- [AWS EKS Setup](/integrations/kubernetes/eks/managed)
- [GCP GKE Setup](/integrations/kubernetes/gke/managed)
- [Azure AKS Setup](/integrations/kubernetes/aks/managed)
- [Scaleway Kapsule Setup](/integrations/kubernetes/scaleway/managed)
- [Bring Your Own Kubernetes](/integrations/kubernetes/byok)
</Info>

<Steps>
  <Step title="Navigate to Clusters">
    In the Qovery Console, go to your **Organization** → **Clusters**

    <Frame>
      <img src="/images/configuration/clusters/cluster_section_access.png" alt="Access Clusters Section" />
    </Frame>
  </Step>

  <Step title="Add New Cluster">
    Click **Create Cluster** and select your cloud provider
  </Step>

  <Step title="Follow Provider-Specific Guide">
    Each cloud provider has unique requirements and configuration options. Follow the appropriate guide:

    <CardGroup cols={2}>
      <Card title="AWS EKS" icon="aws" href="/integrations/kubernetes/eks/managed">
        Create managed EKS cluster
      </Card>
      <Card title="GCP GKE" icon="google" href="/integrations/kubernetes/gke/managed">
        Create managed GKE cluster
      </Card>
      <Card title="Azure AKS" icon="microsoft" href="/integrations/kubernetes/aks/managed">
        Create managed AKS cluster
      </Card>
      <Card title="Scaleway" icon="server" href="/integrations/kubernetes/scaleway/managed">
        Create managed Kapsule cluster
      </Card>
    </CardGroup>
  </Step>
</Steps>

## Cluster Overview

Once your cluster is created, the overview page provides comprehensive information about cluster health, resources, and status.

<Frame>
  <img src="/images/configuration/clusters/cluster-overview.png" alt="Cluster Overview" />
</Frame>

### General Information

**Cluster Details**:
- **Name**: Unique cluster identifier
- **Cloud Provider**: AWS, GCP, Azure, or Scaleway
- **Region**: Geographic location
- **Kubernetes Version**: Current K8s version
- **Status**: Current cluster state
- **Created**: Cluster creation date

### Status Indicators

<Frame>
  <img src="/images/configuration/clusters/cluster_statuses.png" alt="Cluster Status Indicators" />
</Frame>

**Cluster States**:
- <Icon icon="circle-check" iconType="solid" color="green" /> **Running**: Cluster is healthy and operational
- <Icon icon="triangle-exclamation" iconType="solid" color="yellow" /> **Warning**: Minor issues detected, requires attention
- <Icon icon="circle-xmark" iconType="solid" color="red" /> **Error**: Critical issues, intervention required
- <Icon icon="circle" iconType="regular" color="gray" /> **Unavailable**: Cluster unreachable or offline

### Resource Information

View cluster capacity and utilization:

**Metrics Displayed**:
- **Nodes**: Total number of nodes and status
- **CPU**: Total vCPU capacity and current usage
- **Memory**: Total RAM capacity and current usage
- **Pods**: Running pods vs maximum capacity
- **Services**: Number of deployed services

**Example**:
```
Cluster: production-us-east-1
Nodes: 5/10 (auto-scaling enabled)
CPU: 24/40 vCPU (60% utilized)
Memory: 96/160 GB (60% utilized)
Pods: 47/110
Services: 15 applications, 3 databases
```

<Tip>
Monitor resource utilization regularly to optimize costs and ensure adequate capacity. Qovery provides recommendations when resources are over or under-utilized.
</Tip>

### Deployment Status

Track ongoing cluster operations and deployment history:

**Deployment States**:
- **Queued**: Operation waiting to execute
- **In Progress**: Changes being applied
- **Success**: Operation completed successfully
- **Failed**: Operation encountered errors

View detailed logs for each deployment to troubleshoot issues or verify successful operations.

## Cluster Settings

Configure cluster-wide settings and preferences.

### General Settings

**Editable Settings**:
- **Cluster Name**: Update cluster identifier
- **Description**: Add cluster purpose and details
- **Tags**: Organize clusters with custom tags

### Network Configuration

**Network Settings**:
- **VPC/Network**: View VPC/network information
- **CIDR Block**: IP address range for cluster
- **Subnets**: Public and private subnet configuration
- **Load Balancer**: Ingress load balancer settings

<Info>
For advanced networking configurations like VPC peering, see our [AWS VPC Peering Guide](/guides/others/aws-vpc-peering).
</Info>

### Features

**Available Features**:
- **Custom Domain**: Configure wildcard domains for services
- **Static IP**: Reserve static IP addresses
- **Private Registry**: Configure private container registries
- **Observability**: Enable monitoring and logging integrations

### Advanced Settings

Cluster Advanced Settings provide fine-grained control over cluster infrastructure, networking, logging, and cloud provider-specific configurations. These settings allow you to optimize performance, security, and cost without requiring direct cloud provider console access.

<Info>
Advanced settings are configured via the **Qovery API**. Console UI support is coming soon.
</Info>

<Warning>
Some advanced settings may impact cluster stability or cost. Review documentation carefully before making changes. Settings marked with ⚠️ warnings can cause cluster instability if misconfigured.
</Warning>

## Cloud Provider Legend

Advanced settings apply to different cloud providers:

| Icon | Provider |
|------|----------|
| ![AWS](/images/advanced_settings/aws.svg) | Amazon Web Services |
| ![GCP](/images/advanced_settings/gcp.svg) | Google Cloud Platform |
| ![Azure](/images/advanced_settings/azure.svg) | Microsoft Azure |
| ![Scaleway](/images/advanced_settings/scaleway.svg) | Scaleway |

## Logs Settings

Configure log retention and flow logs for your cluster.

### aws.cloudwatch.eks_logs_retention_days

Maximum retention days in CloudWatch for EKS logs.

- **Default**: `90` days
- **Type**: Integer
- **Cloud Provider**: AWS
- **Possible Values**: 0, 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 2192, 2557, 2922, 3288, 3653

```json
{
  "aws.cloudwatch.eks_logs_retention_days": 180
}
```

### aws.vpc.enable_s3_flow_logs

Enable VPC flow logs and store them in S3.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Provider**: AWS

```json
{
  "aws.vpc.enable_s3_flow_logs": true
}
```

### aws.vpc.flow_logs_retention_days

Number of retention days for VPC flow logs.

- **Default**: `365` days
- **Type**: Integer
- **Cloud Provider**: AWS

<Info>
Set to `0` for unlimited retention.
</Info>

```json
{
  "aws.vpc.flow_logs_retention_days": 0
}
```

### loki.log_retention_in_week

Maximum Kubernetes pod log retention in weeks (Loki).

- **Default**: `12` weeks (84 days)
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "loki.log_retention_in_week": 24
}
```

### gcp.vpc.enable_flow_logs

Enable VPC flow logs on all VPC subnetworks.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Provider**: GCP

```json
{
  "gcp.vpc.enable_flow_logs": true
}
```

### gcp.vpc.flow_logs_sampling

VPC flow logs sampling percentage.

- **Default**: `0.0` (no sampling)
- **Type**: Float (0.0 to 1.0)
- **Cloud Provider**: GCP

<Info>
`0.0` = no sampling, `1.0` = capture all logs
</Info>

```json
{
  "gcp.vpc.flow_logs_sampling": 0.5
}
```

## DNS Settings

Configure custom DNS resolution rules.

### dns.coredns.extra_config

Add additional CoreDNS configuration for custom DNS resolution.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, Scaleway, Azure

**Example**: Forward `example.com` queries to custom DNS server.

```json
{
  "dns.coredns.extra_config": "example.com:53 {\n  errors\n  cache 30\n  forward . 8.8.8.8\n}"
}
```

## Image Registry Settings

Configure container image registry behavior.

### registry.image_retention_time

Time in seconds before images in default registry are deleted.

- **Default**: `31536000` (1 year)
- **Type**: Integer (seconds)
- **Cloud Provider**: AWS

<Note>
Only affects new ECR repositories created after this setting is changed.
</Note>

```json
{
  "registry.image_retention_time": 63072000
}
```

### registry.mirroring_mode

Image mirroring mode for cluster deployments.

- **Default**: `"Service"`
- **Type**: String
- **Values**: `"Service"` or `"Cluster"`
- **Cloud Provider**: AWS

```json
{
  "registry.mirroring_mode": "Cluster"
}
```

### cloud_provider.container_registry.tags

Add custom tags to cluster-dedicated registry.

- **Default**: Not specified
- **Type**: Map (String keys and values)
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "cloud_provider.container_registry.tags": {
    "Team": "platform",
    "Environment": "production"
  }
}
```

## Network Settings

Configure load balancers, Nginx controller, and networking behavior.

### Load Balancer (Scaleway)

#### load_balancer.size

Specify load balancer size for Scaleway clusters.

- **Default**: `"lb-s"`
- **Type**: String
- **Cloud Provider**: Scaleway
- **Values**:
  - `"lb-s"`: 200 Mbps
  - `"lb-gp-m"`: 500 Mbps
  - `"lb-gp-l"`: 1 Gbps
  - `"lb-gp-xl"`: 4 Gbps

```json
{
  "load_balancer.size": "lb-gp-l"
}
```

### AWS Load Balancer Controller

#### aws.eks.enable_alb_controller

Enable AWS Application Load Balancer (ALB) controller.

- **Default**: `true`
- **Type**: Boolean
- **Cloud Provider**: AWS

```json
{
  "aws.eks.enable_alb_controller": false
}
```

#### aws.eks.alb_controller.vpa.vcpu.min_in_milli_cpu

AWS ALB controller VPA minimum vCPU.

- **Default**: `250` millicores
- **Type**: Integer
- **Cloud Provider**: AWS

```json
{
  "aws.eks.alb_controller.vpa.vcpu.min_in_milli_cpu": 500
}
```

#### aws.eks.alb_controller.vpa.vcpu.max_in_milli_cpu

AWS ALB controller VPA maximum vCPU.

- **Default**: `250` millicores
- **Type**: Integer
- **Cloud Provider**: AWS

```json
{
  "aws.eks.alb_controller.vpa.vcpu.max_in_milli_cpu": 1000
}
```

#### aws.eks.alb_controller.vpa.memory.min_in_mib

AWS ALB controller VPA minimum memory.

- **Default**: `128` MiB
- **Type**: Integer
- **Cloud Provider**: AWS

```json
{
  "aws.eks.alb_controller.vpa.memory.min_in_mib": 256
}
```

#### aws.eks.alb_controller.vpa.memory.max_in_mib

AWS ALB controller VPA maximum memory.

- **Default**: `128` MiB
- **Type**: Integer
- **Cloud Provider**: AWS

```json
{
  "aws.eks.alb_controller.vpa.memory.max_in_mib": 512
}
```

### Nginx Controller Resources

#### nginx.vcpu.request_in_milli_cpu

Nginx controller vCPU request.

- **Default**: `200` millicores
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.vcpu.request_in_milli_cpu": 500
}
```

#### nginx.vcpu.limit_in_milli_cpu

Nginx controller vCPU limit.

- **Default**: `700` millicores
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.vcpu.limit_in_milli_cpu": 1000
}
```

#### nginx.memory.request_in_mib

Nginx controller memory request.

- **Default**: `768` MiB
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.memory.request_in_mib": 1024
}
```

#### nginx.memory.limit_in_mib

Nginx controller memory limit.

- **Default**: `768` MiB
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.memory.limit_in_mib": 1024
}
```

### Nginx Controller HPA

#### nginx.hpa.cpu_utilization_percentage_threshold

HPA CPU threshold for Nginx deployment.

- **Default**: `50`%
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.hpa.cpu_utilization_percentage_threshold": 70
}
```

#### nginx.hpa.min_number_instances

Minimum number of Nginx instances.

- **Default**: `2`
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.hpa.min_number_instances": 3
}
```

#### nginx.hpa.max_number_instances

Maximum number of Nginx instances.

- **Default**: `25`
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.hpa.max_number_instances": 50
}
```

### Nginx Controller Configuration

#### nginx.controller.enable_client_ip

Enable `ngx_http_realip_module` to pass client IP.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.enable_client_ip": true
}
```

#### nginx.controller.enable_compression

Enable Brotli compression for HTTP responses.

- **Default**: `true`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.enable_compression": false
}
```

#### nginx.controller.use_forwarded_headers

Pass incoming `X-Forwarded-For` header upstream.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.use_forwarded_headers": true
}
```

#### nginx.controller.compute_full_forwarded_for

Append remote address to `X-Forwarded-For` header.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.compute_full_forwarded_for": true
}
```

#### nginx.controller.log_format_upstream

Customize Nginx log format.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.log_format_upstream": "$remote_addr - $remote_user [$time_local] \"$request\" $status"
}
```

#### nginx.controller.log_format_escaping

Customize Nginx log format escaping.

- **Default**: `"Default"`
- **Type**: String
- **Values**: `"Default"`, `"JSON"`, `"None"`
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.log_format_escaping": "JSON"
}
```

#### nginx.controller.http_snippet

Add custom Nginx HTTP snippet configuration.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.http_snippet": "proxy_cache_path /tmp/cache keys_zone=cache:10m;"
}
```

#### nginx.controller.server_snippet

Add custom Nginx server snippet configuration.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.server_snippet": "location /health { return 200; }"
}
```

#### nginx.controller.limit_request_status_code

Customize Nginx rate limit status code.

- **Default**: `null` (Nginx defaults to 503)
- **Type**: Integer
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.limit_request_status_code": 429
}
```

#### nginx.controller.custom_http_errors

Customize Nginx custom HTTP error codes.

- **Default**: `null`
- **Type**: String (comma-separated)
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.controller.custom_http_errors": "404,503"
}
```

### Nginx Default Backend

#### nginx.default_backend.enabled

Enable Nginx default backend.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

<Note>
Ensure cluster has nodes with amd64 architecture.
</Note>

```json
{
  "nginx.default_backend.enabled": true
}
```

#### nginx.default_backend.image_repository

Docker image repository for default backend.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

<Info>
Image must be publicly accessible.
</Info>

```json
{
  "nginx.default_backend.image_repository": "myregistry/custom-backend"
}
```

#### nginx.default_backend.image_tag

Image tag for default backend.

- **Default**: `null`
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "nginx.default_backend.image_tag": "1.4"
}
```

## Service Settings

Control resource overcommit and database access.

### allow_service_cpu_overcommit

Allow CPU overcommit (limit > request) for services.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

<Warning>
⚠️ Enabling CPU overcommit can lead to cluster instability and unpredictable performance.
</Warning>

```json
{
  "allow_service_cpu_overcommit": true
}
```

### allow_service_ram_overcommit

Allow memory overcommit (limit > request) for services.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

<Warning>
⚠️ Enabling memory overcommit can lead to cluster instability, pod evictions, and OOM errors.
</Warning>

```json
{
  "allow_service_ram_overcommit": true
}
```

### Database Access Control

Configure network access restrictions for managed and container databases.

**Database Icons**:
- ![Database Container](/images/advanced_settings/database-container.svg) Container Database
- ![Database Managed](/images/advanced_settings/database-managed.svg) Managed Database

#### database.postgresql.deny_any_access

Deny all access to PostgreSQL databases.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.postgresql.deny_any_access": true
}
```

#### database.postgresql.allowed_cidrs

List of allowed CIDR ranges for PostgreSQL.

- **Default**: `["0.0.0.0/0"]`
- **Type**: Array of strings
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.postgresql.allowed_cidrs": ["10.0.0.0/8", "172.16.0.0/12"]
}
```

#### database.mysql.deny_any_access

Deny all access to MySQL databases.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.mysql.deny_any_access": true
}
```

#### database.mysql.allowed_cidrs

List of allowed CIDR ranges for MySQL.

- **Default**: `["0.0.0.0/0"]`
- **Type**: Array of strings
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.mysql.allowed_cidrs": ["10.0.0.0/8"]
}
```

#### database.mongodb.deny_any_access

Deny all access to MongoDB databases.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.mongodb.deny_any_access": true
}
```

#### database.mongodb.allowed_cidrs

List of allowed CIDR ranges for MongoDB.

- **Default**: `["0.0.0.0/0"]`
- **Type**: Array of strings
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.mongodb.allowed_cidrs": ["10.0.0.0/8"]
}
```

#### database.redis.deny_any_access

Deny all access to Redis databases.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.redis.deny_any_access": true
}
```

#### database.redis.allowed_cidrs

List of allowed CIDR ranges for Redis.

- **Default**: `["0.0.0.0/0"]`
- **Type**: Array of strings
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "database.redis.allowed_cidrs": ["10.0.0.0/8"]
}
```

## IAM Settings (AWS)

Configure AWS IAM integration for cluster access.

### aws.iam.enable_admin_group_sync

Enable IAM admin group synchronization.

- **Default**: `true`
- **Type**: Boolean
- **Cloud Provider**: AWS

<Note>
Requires `aws.iam.admin_group` to be set.
</Note>

```json
{
  "aws.iam.enable_admin_group_sync": false
}
```

### aws.iam.admin_group

IAM group name associated with Qovery users in AWS console.

- **Default**: `"Admins"`
- **Type**: String
- **Cloud Provider**: AWS

<Info>
Can be changed after cluster installation.
</Info>

```json
{
  "aws.iam.admin_group": "QoveryAdministrators"
}
```

### aws.iam.enable_sso

Enable SSO sync for IAM users.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Provider**: AWS

<Note>
Requires `aws.iam.sso_role_arn` to be set.
</Note>

```json
{
  "aws.iam.enable_sso": true
}
```

### aws.iam.sso_role_arn

SSO role ARN for cluster access.

- **Default**: `""` (empty)
- **Type**: String
- **Cloud Provider**: AWS

```json
{
  "aws.iam.sso_role_arn": "arn:aws:iam::123456789012:role/QoverySSORole"
}
```

## Object Storage

Configure logging for cluster object storage buckets.

### object_storage.enable_logging

Enable logging for cluster buckets into `{bucket-name}-log` bucket.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP, Azure

**Documentation**:
- [AWS S3 Logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html)
- [GCP Cloud Logging](https://cloud.google.com/logging/docs/buckets)

```json
{
  "object_storage.enable_logging": true
}
```

## Miscellaneous Settings

Additional cluster configuration options.

### storageclass.fast_ssd

Specify Kubernetes storageClass for container databases and applications.

- **Default**: Varies by cloud provider
- **Type**: String
- **Cloud Providers**: AWS, GCP, Azure, Scaleway

```json
{
  "storageclass.fast_ssd": "gp3"
}
```

### aws.eks.ec2.metadata_imds

Specify EC2 Instance Metadata Service (IMDS) version.

- **Default**: `"required"` (IMDS v2)
- **Type**: String
- **Values**: `"required"` (IMDS v2 only), `"optional"` (IMDS v1 and v2)
- **Cloud Provider**: AWS

```json
{
  "aws.eks.ec2.metadata_imds": "optional"
}
```

### aws.eks.ec2.ami

Specify the Amazon Machine Image (AMI) for EKS nodes.

- **Default**: `"AmazonLinux2023"`
- **Type**: String
- **Values**: `"AmazonLinux2"` (deprecated), `"AmazonLinux2023"`, `"Bottlerocket"`
- **Cloud Provider**: AWS

<Warning>
`AmazonLinux2` is deprecated and won't work after Kubernetes 1.32.
</Warning>

```json
{
  "aws.eks.ec2.ami": "Bottlerocket"
}
```

### aws.eks.encrypt_secrets_kms_key_arn

Activate KMS encryption of Kubernetes secrets.

- **Default**: `null`
- **Type**: String (ARN)
- **Cloud Provider**: AWS

<Warning>
Cannot be reversed once activated.
</Warning>

```json
{
  "aws.eks.encrypt_secrets_kms_key_arn": "arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012"
}
```

### qovery.static_ip_mode

Enable static IP mode for Qovery control plane.

- **Default**: `false`
- **Type**: Boolean
- **Cloud Providers**: AWS, GCP

<Note>
Requires Docker Hub credentials.
</Note>

```json
{
  "qovery.static_ip_mode": true
}
```

### k8s.api.allowed_public_access_cidrs

Whitelist additional CIDRs to access Kubernetes API.

- **Default**: `[]` (empty array)
- **Type**: Array of strings
- **Cloud Providers**: AWS, GCP, Azure

<Info>
Requires `qovery.static_ip_mode = true`.
</Info>

```json
{
  "k8s.api.allowed_public_access_cidrs": ["203.0.113.0/24", "198.51.100.0/24"]
}
```

## Configuring Advanced Settings via API

Advanced settings are configured using the Qovery API:

```bash
curl -X PATCH "https://api.qovery.com/cluster/{cluster_id}/advancedSettings" \
  -H "Authorization: Token YOUR_API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "loki.log_retention_in_week": 24,
    "nginx.hpa.min_number_instances": 3,
    "database.postgresql.allowed_cidrs": ["10.0.0.0/8"]
  }'
```

<Info>
For complete API documentation, see the [Qovery API Reference](/api-reference/introduction).
</Info>

## Advanced Settings Best Practices

<AccordionGroup>
  <Accordion title="Document Changes">
    Keep track of all advanced settings changes with rationale. Cluster settings can significantly impact performance and cost.
  </Accordion>

  <Accordion title="Test in Non-Production">
    Always test advanced settings in development or staging clusters before applying to production.
  </Accordion>

  <Accordion title="Avoid Overcommit in Production">
    Never enable `allow_service_cpu_overcommit` or `allow_service_ram_overcommit` in production clusters. This can cause unpredictable behavior and pod evictions.
  </Accordion>

  <Accordion title="Monitor After Changes">
    After changing advanced settings, monitor cluster health, resource utilization, and service performance closely for 24-48 hours.
  </Accordion>

  <Accordion title="Use Database Access Control">
    Restrict database access with `allowed_cidrs` instead of allowing `0.0.0.0/0`. This significantly improves security posture.
  </Accordion>

  <Accordion title="Optimize Nginx Resources">
    Tune Nginx controller resources based on traffic patterns. Monitor HPA scaling behavior and adjust thresholds accordingly.
  </Accordion>

  <Accordion title="Enable Flow Logs for Security">
    Enable VPC flow logs for production clusters to aid in security audits and troubleshooting network issues.
  </Accordion>
</AccordionGroup>

## Cluster Actions

<Frame>
  <img src="/images/configuration/clusters/cluster_actions.png" alt="Cluster Actions" />
</Frame>

### Update Cluster

Modify cluster configuration without recreating it:

**Updatable Components**:
- Node instance types
- Number of nodes (min/max for auto-scaling)
- Kubernetes add-ons
- Network policies
- Security settings

<Steps>
  <Step title="Access Settings">
    Navigate to **Cluster** → **Settings**
  </Step>

  <Step title="Modify Configuration">
    Update desired settings (instance types, scaling, etc.)
  </Step>

  <Step title="Review Changes">
    Review the proposed changes and estimated impact
  </Step>

  <Step title="Apply Update">
    Click **Update** to apply changes with rolling updates
  </Step>
</Steps>

<Info>
Most updates are performed with zero downtime using rolling node replacements. Your services remain available during the update.
</Info>

### Stop Cluster

Temporarily stop a cluster to reduce costs:

<Steps>
  <Step title="Stop Cluster">
    Click **Actions** → **Stop Cluster**
  </Step>

  <Step title="Confirm">
    Confirm the action. All services will be stopped.
  </Step>
</Steps>

**What Happens**:
- All running services are stopped gracefully
- Nodes are terminated
- Persistent data remains intact
- Cloud costs reduced to storage only

**To Restart**: Use **Actions** → **Start Cluster** to restore the cluster and redeploy services.

<Tip>
Stop development and staging clusters during off-hours to significantly reduce infrastructure costs.
</Tip>

### Delete Cluster

Permanently remove a cluster and its resources:

<Warning>
Cluster deletion is **permanent and irreversible**. All services, data, and configurations will be lost unless backed up separately.
</Warning>

<Steps>
  <Step title="Prepare for Deletion">
    - Backup important data and configurations
    - Export Terraform state if using IaC
    - Document any custom configurations
    - Stop all services (recommended)
  </Step>

  <Step title="Initiate Deletion">
    Click **Actions** → **Delete Cluster**
  </Step>

  <Step title="Choose Deletion Mode">
    Select one of the following:

    **Option 1: Delete Everything** (Recommended)
    - Removes all Qovery-managed resources
    - Deletes cloud provider infrastructure
    - Complete cleanup with no residual costs

    **Option 2: Keep Cloud Resources**
    - Removes Qovery management only
    - Keeps cloud infrastructure running
    - Continue managing manually in cloud console
  </Step>

  <Step title="Confirm Deletion">
    Type the cluster name to confirm deletion
  </Step>
</Steps>

**Deletion Process**:
1. Stop all running services
2. Delete Kubernetes workloads
3. Terminate cluster nodes
4. Remove load balancers and networking
5. Delete persistent volumes (if selected)
6. Clean up cloud provider resources

<Tip>
Before deletion, export your configuration using Qovery Terraform Provider or CLI for potential future recreation.
</Tip>

### View Logs

Access cluster infrastructure logs for troubleshooting:

<Frame>
  <img src="/images/configuration/clusters/cluster_logs_access.png" alt="Access Cluster Logs" />
</Frame>

<Steps>
  <Step title="Access Logs">
    Click **Actions** → **View Logs**
  </Step>

  <Step title="Select Log Type">
    Choose log category:
    - **Infrastructure**: Cluster provisioning and management
    - **Kubernetes**: K8s control plane events
    - **Network**: Load balancer and ingress logs
  </Step>

  <Step title="Filter and Search">
    - Filter by time range
    - Search by keyword
    - Filter by severity level
  </Step>
</Steps>

**Successful Operations**:

<Frame>
  <img src="/images/configuration/clusters/ok-infra-logs.jpg" alt="Successful Cluster Logs" />
</Frame>

**Error Logs**:

<Frame>
  <img src="/images/configuration/clusters/error-infra-logs.jpg" alt="Cluster Error Logs" />
</Frame>

### Access Kubeconfig

Download Kubernetes configuration for direct cluster access:

<Steps>
  <Step title="Download Kubeconfig">
    Click **Actions** → **Download Kubeconfig**
  </Step>

  <Step title="Save Configuration">
    Save the kubeconfig file:
    ```bash
    # Default location
    mv ~/Downloads/kubeconfig ~/.kube/config

    # Or specify custom location
    export KUBECONFIG=~/path/to/kubeconfig
    ```
  </Step>

  <Step title="Verify Access">
    Test cluster access:
    ```bash
    kubectl cluster-info
    kubectl get nodes
    kubectl get pods --all-namespaces
    ```
  </Step>
</Steps>

<Warning>
**Security Best Practices**:
- Kubeconfig provides cluster-admin access
- Never commit kubeconfig to version control
- Store securely (use secret management tools)
- Rotate credentials regularly
- Use RBAC for team member access instead
</Warning>

<Info>
Direct kubectl access should be used sparingly. Qovery Console provides safer interfaces for most operations. Use kubectl primarily for debugging or advanced scenarios not covered by the Console.
</Info>

## Kubernetes Version Management

### Current Version

View your cluster's Kubernetes version in the cluster overview:

**Version Information**:
- **Current Version**: Currently running K8s version
- **Latest Available**: Newest supported version
- **End of Support**: When current version reaches EOL

### Upgrade Process

Qovery manages Kubernetes upgrades to ensure security and stability:

**Managed Upgrade Benefits**:
- Automated pre-upgrade health checks
- Rolling node upgrades with zero downtime
- Service health monitoring during upgrade
- Automatic rollback on failure
- Post-upgrade verification

<Steps>
  <Step title="Notification">
    Qovery notifies you when new versions are available
  </Step>

  <Step title="Review Release Notes">
    Check Kubernetes changelog and breaking changes
  </Step>

  <Step title="Test in Staging">
    Upgrade staging/development clusters first
  </Step>

  <Step title="Schedule Production Upgrade">
    Choose low-traffic window for production upgrades
  </Step>

  <Step title="Monitor Upgrade">
    Watch upgrade progress and service health
  </Step>
</Steps>

<Warning>
**Never upgrade Kubernetes manually** through your cloud provider console. This can cause incompatibilities with Qovery components and service disruptions. Always use Qovery-managed upgrades.
</Warning>

### Version Support Policy

**Qovery Support**:
- Latest 3 Kubernetes minor versions fully supported
- Security patches applied automatically
- 3-month notice before version deprecation
- Forced upgrades only after extended grace period

**Example Timeline**:
```
Version 1.28 Released → Fully Supported (12 months)
Version 1.29 Released → 1.28 still supported (9 months remaining)
Version 1.30 Released → 1.28 supported (6 months remaining)
Version 1.31 Released → 1.28 deprecated (upgrade recommended)
Version 1.32 Released → 1.28 end-of-support (forced upgrade)
```

## Security and Compliance

### Vulnerability Management

Qovery provides comprehensive security monitoring:

**Automated Security**:
- Continuous vulnerability scanning
- CVE database integration
- Real-time security alerts
- Severity classification (Critical, High, Medium, Low)

**What's Monitored**:
- Kubernetes control plane components
- Node operating system packages
- Container runtime
- Cluster add-ons and plugins

### Security Patching

When vulnerabilities are detected:

1. **Assessment**: Qovery evaluates impact and severity
2. **Patch Preparation**: Security patches are tested
3. **Notification**: Customers receive detailed advisories
4. **Guided Remediation**: Step-by-step instructions provided
5. **Automated Patching**: Critical issues auto-patched (optional)

### Compliance Features

**Security Capabilities**:
- Network policies and isolation
- RBAC and access controls
- Audit logging
- Secrets management
- Image scanning
- SOC 2 compliance

<Info>
Learn more about Qovery's security and compliance certifications in our [Security Overview](/security-and-compliance/overview).
</Info>

## Monitoring and Observability

### Built-in Metrics

Qovery provides cluster-level monitoring:

**Infrastructure Metrics**:
- Node CPU and memory utilization
- Disk I/O and network throughput
- Pod resource consumption
- Control plane health

**Accessing Metrics**:
- Navigate to **Cluster** → **Monitoring**
- Select time range (1h, 6h, 24h, 7d, 30d)
- Analyze trends and identify bottlenecks

### Alerting

Configure alerts for cluster issues:

**Alert Types**:
- High CPU usage (>80%)
- High memory usage (>85%)
- Node unavailable
- Disk space critical (less than 20%)
- Control plane issues

**Notification Channels**:
- Email
- Slack
- Webhooks
- PagerDuty
- Custom integrations

### External Observability

Integrate with external monitoring solutions:

<CardGroup cols={2}>
  <Card title="Datadog" icon="chart-line" href="/integrations/observability/datadog">
    Full-stack monitoring and APM
  </Card>
  <Card title="New Relic" icon="chart-area" href="/integrations/observability/new-relic">
    Application performance monitoring
  </Card>
  <Card title="Qovery Observe" icon="magnifying-glass-chart" href="/guides/qovery-101/observe">
    Qovery's native observability platform
  </Card>
  <Card title="Other Solutions" icon="plug" href="/integrations/observability/overview">
    Prometheus, Grafana, and more
  </Card>
</CardGroup>

## Multi-Cluster Management

Manage multiple clusters across regions and cloud providers:

### Common Patterns

**Geographic Distribution**:
```
Cluster 1: AWS us-east-1 (Production - North America)
Cluster 2: AWS eu-west-1 (Production - Europe)
Cluster 3: AWS ap-southeast-1 (Production - Asia Pacific)
```

**Environment Separation**:
```
Cluster 1: Production (isolated, high availability)
Cluster 2: Staging + QA
Cluster 3: Development + Preview Environments
```

**Multi-Cloud Strategy**:
```
Cluster 1: AWS EKS (Primary)
Cluster 2: GCP GKE (Disaster recovery)
Cluster 3: Azure AKS (Region-specific compliance)
```

### Best Practices

<AccordionGroup>
  <Accordion title="Naming Conventions">
    Use consistent, descriptive cluster names:
    ```
    Format: {environment}-{provider}-{region}
    Examples:
    - production-aws-us-east-1
    - staging-gcp-europe-west1
    - dev-scaleway-par1
    ```
  </Accordion>

  <Accordion title="Cluster Tagging">
    Tag clusters for organization and cost tracking:
    - **Environment**: production, staging, development
    - **Team**: backend, frontend, data
    - **Cost Center**: engineering, operations
    - **Purpose**: web-apps, ml-workloads, data-processing
  </Accordion>

  <Accordion title="Access Management">
    Control cluster access with RBAC:
    - Limit cluster-admin access
    - Use team-based permissions
    - Implement least-privilege access
    - Audit access regularly
  </Accordion>

  <Accordion title="Cross-Cluster Networking">
    For services spanning multiple clusters:
    - Use VPC/VNet peering
    - Implement service mesh (Istio, Linkerd)
    - Configure cross-cluster DNS
    - Set up global load balancing
  </Accordion>
</AccordionGroup>

## Best Practices

<CardGroup cols={2}>
  <Card title="Right-Size Your Cluster" icon="gauge-high" href="/configuration/clusters">
    - Start with smaller instances
    - Enable auto-scaling for flexibility
    - Monitor utilization regularly
    - Leave 30% capacity buffer for traffic spikes
  </Card>

  <Card title="Kubernetes Versions" icon="code-branch" href="/configuration/clusters">
    - Never manually upgrade from cloud console
    - Stay within supported version range
    - Upgrade non-production first
    - Review release notes before upgrading
  </Card>

  <Card title="High Availability" icon="shield-check" href="/configuration/clusters">
    - Use minimum 3 nodes for production
    - Enable multi-AZ/zone deployment
    - Implement pod disruption budgets
    - Test failover scenarios regularly
  </Card>

  <Card title="Cost Optimization" icon="piggy-bank" href="/configuration/clusters">
    - Right-size node instance types
    - Use spot/preemptible instances where safe
    - Stop dev/test clusters after hours
    - Review and optimize resource requests
  </Card>

  <Card title="Security" icon="lock" href="/security-and-compliance/overview">
    - Rotate kubeconfig credentials regularly
    - Limit cluster-admin access
    - Enable audit logging
    - Review security advisories promptly
  </Card>

  <Card title="Monitoring" icon="chart-mixed" href="/configuration/monitoring">
    - Set up critical alerts
    - Review cluster health weekly
    - Monitor costs and trends
    - Document baseline performance
  </Card>
</CardGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Cluster Shows Warning Status">
    **Symptoms**: Yellow warning indicator on cluster

    **Common Causes**:
    - One or more nodes unhealthy
    - High resource utilization (>80%)
    - Network connectivity issues
    - Recent failed deployments

    **Resolution Steps**:
    1. Check cluster overview for specific warnings
    2. Review node status: `kubectl get nodes`
    3. Check resource utilization in monitoring
    4. Review recent deployment logs
    5. Restart unhealthy nodes if needed
    6. Scale cluster if resources are constrained
  </Accordion>

  <Accordion title="Cluster Creation Failed">
    **Common Issues**:
    - Insufficient cloud provider quotas
    - Invalid or expired credentials
    - Region capacity limitations
    - Network configuration errors
    - Exceeded account limits

    **Resolution**:
    1. Check deployment logs for specific error message
    2. Verify cloud credentials are valid and have permissions
    3. Check quota limits in cloud provider console
    4. Try alternative region or instance type
    5. Contact Qovery support with cluster ID
  </Accordion>

  <Accordion title="Services Won't Deploy">
    **Check**:
    - Cluster status is Running (green)
    - Sufficient CPU/memory available
    - Image pull secrets configured
    - No network policy conflicts
    - Container registry accessible

    **Debug Steps**:
    ```bash
    # Check cluster nodes
    kubectl get nodes

    # Check pod events
    kubectl describe pod POD_NAME -n NAMESPACE

    # Check resource availability
    kubectl top nodes

    # Check container logs
    kubectl logs POD_NAME -n NAMESPACE
    ```
  </Accordion>

  <Accordion title="Cannot Access with kubectl">
    **Troubleshooting**:

    1. **Verify kubeconfig**:
       ```bash
       kubectl cluster-info
       ```

    2. **Check kubectl version**:
       ```bash
       kubectl version --client
       ```

    3. **Test connectivity**:
       ```bash
       kubectl get nodes
       ```

    4. **Common errors**:
       - `Unable to connect`: Check network/VPN connection
       - `Unauthorized`: Download fresh kubeconfig from Qovery
       - `Certificate expired`: Kubeconfig may need rotation

    5. **Download fresh kubeconfig** from Qovery Console
  </Accordion>

  <Accordion title="Cluster Status Unavailable">
    <Frame>
      <img src="/images/configuration/clusters/cluster_status_unavailable.png" alt="Cluster Unavailable Status" />
    </Frame>

    **Symptoms**: Gray/unavailable status indicator

    **Possible Causes**:
    - Cloud provider outage
    - Network connectivity lost
    - Control plane issues
    - Cluster manually modified outside Qovery

    **Resolution**:
    1. Check cloud provider status page
    2. Verify cloud credentials haven't expired
    3. Check Qovery status page
    4. Review cluster logs in Qovery Console
    5. Contact Qovery support if issue persists
  </Accordion>

  <Accordion title="High Cloud Costs">
    **Cost Optimization Checklist**:

    ✅ **Node Optimization**:
    - Review instance types (downsize if possible)
    - Enable auto-scaling to scale down during low usage
    - Use spot/preemptible instances for non-critical workloads

    ✅ **Resource Optimization**:
    - Review application resource requests/limits
    - Identify idle or underutilized services
    - Delete unused persistent volumes

    ✅ **Scheduling**:
    - Stop dev/test clusters outside business hours
    - Schedule non-critical batch jobs during off-peak

    ✅ **Monitoring**:
    - Enable cost allocation tags
    - Review cloud provider cost explorer
    - Set up budget alerts
  </Accordion>
</AccordionGroup>

## Cloud Provider Cleanup

If cluster deletion fails or leaves resources behind:

<Frame>
  <img src="/images/configuration/clusters/aws-console-cluster-cleanup.jpg" alt="Manual Cluster Cleanup in AWS Console" />
</Frame>

<Warning>
Manual cleanup should only be done if Qovery deletion fails. This can lead to orphaned resources or unexpected costs if not done carefully.
</Warning>

**Manual Cleanup Steps** (AWS Example):
1. Delete EKS cluster in AWS Console
2. Delete associated node groups
3. Delete VPC and subnets (if dedicated)
4. Delete load balancers
5. Delete security groups
6. Delete EBS volumes
7. Verify all resources deleted to avoid lingering costs

<Info>
For other cloud providers (GCP, Azure, Scaleway), refer to their respective console documentation for resource cleanup.
</Info>

## Next Steps

<CardGroup cols={2}>
  <Card title="Create Environment" icon="layer-group" href="/configuration/environment">
    Set up your first environment on the cluster
  </Card>
  <Card title="Deploy Application" icon="rocket" href="/guides/getting-started/deploy-your-first-application">
    Deploy your first application
  </Card>
  <Card title="AWS EKS Integration" icon="aws" href="/integrations/kubernetes/eks/overview">
    Learn more about AWS EKS clusters
  </Card>
  <Card title="Multi-Cluster Management" icon="server" href="/guides/use-cases/multi-kubernetes-clusters-management">
    Advanced multi-cluster strategies
  </Card>
</CardGroup>
