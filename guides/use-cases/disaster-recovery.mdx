---
title: "Disaster Recovery"
description: "Implement robust disaster recovery strategies with Qovery for business continuity and minimal downtime"
---

## Overview

Disaster recovery (DR) is a critical component of infrastructure management that ensures business continuity in the face of unexpected outages, failures, or disasters. Qovery provides comprehensive tools and features to implement effective DR strategies across multiple cloud providers and regions.

## What is Disaster Recovery?

Disaster recovery encompasses the policies, tools, and procedures to recover or continue vital technology infrastructure and systems following a natural or human-induced disaster. For cloud-based applications, this includes:

- **Infrastructure Failures**: Cloud provider outages, data center issues
- **Data Loss**: Accidental deletions, corruption, ransomware attacks
- **Human Errors**: Configuration mistakes, deployment issues
- **Natural Disasters**: Earthquakes, floods, fires affecting data centers
- **Security Incidents**: DDoS attacks, data breaches, system compromises

## Key DR Metrics

Understanding these metrics helps define your disaster recovery requirements:

### Recovery Time Objective (RTO)

**Definition**: Maximum acceptable time to restore services after a disaster

**Qovery RTO Examples**:
- **Database Restore**: 15-30 minutes for databases under 100GB
- **Environment Recreation**: 20-40 minutes for full environment deployment
- **Multi-Region Failover**: 5-15 minutes with automated failover
- **Application Redeployment**: 5-10 minutes per service

### Recovery Point Objective (RPO)

**Definition**: Maximum acceptable data loss measured in time

**Qovery RPO Examples**:
- **Database Backups**: 24 hours (standard), < 1 minute (continuous backup)
- **Configuration State**: Real-time (Terraform/Git-backed)
- **Application State**: Depends on your backup strategy
- **Infrastructure as Code**: Zero data loss (version controlled)

<Info>
Qovery enables RTO/RPO values that align with your business requirements, from basic DR (24h RPO/RTO) to mission-critical setups (< 1 minute RPO/RTO).
</Info>

## Disaster Recovery Strategies

Qovery supports multiple DR strategies, each with different complexity, cost, and recovery characteristics:

### 1. Backup and Restore (Lowest Cost)

**Strategy**: Regular backups with manual restoration process

**Implementation with Qovery**:
- Automated daily database backups
- Configuration exported to Terraform
- Container images stored in registry
- Infrastructure as Code in Git

**Recovery Process**:
1. Provision new cluster in different region
2. Restore databases from backups
3. Deploy applications from container registry
4. Update DNS to new cluster

**Characteristics**:
- **RTO**: 2-4 hours
- **RPO**: 24 hours
- **Cost**: Lowest (backup storage only)
- **Complexity**: Low
- **Use Case**: Non-critical applications, development environments

<Steps>
  <Step title="Set Up Automated Backups">
    Configure daily backups for all databases:
    ```bash
    # Via Qovery Console
    Database → Settings → Backups → Enable Daily Backups
    Retention: 30 days
    ```
  </Step>

  <Step title="Export Configuration">
    Export infrastructure configuration:
    ```bash
    # Export environment as Terraform
    qovery environment export --environment production --format terraform

    # Commit to Git
    git add terraform/
    git commit -m "Backup production configuration"
    git push
    ```
  </Step>

  <Step title="Test Restore Procedure">
    Quarterly DR test:
    ```bash
    # Create test environment
    qovery environment clone --from production --to dr-test

    # Restore database from backup
    qovery database restore --database-id <db-id> --backup-id <backup-id>

    # Verify application functionality
    ```
  </Step>
</Steps>

### 2. Pilot Light (Low Cost, Faster Recovery)

**Strategy**: Minimal infrastructure always running, scale up during disaster

**Implementation with Qovery**:
- Secondary cluster with minimal nodes
- Databases replicated with lower instance types
- Applications deployed but scaled to 0 replicas
- DNS ready to switch

**Recovery Process**:
1. Scale up DR cluster nodes
2. Scale applications to production replicas
3. Promote read-replica databases to primary
4. Switch DNS to DR cluster

**Characteristics**:
- **RTO**: 30 minutes - 1 hour
- **RPO**: 5-15 minutes (database replication lag)
- **Cost**: Low (minimal infrastructure)
- **Complexity**: Medium
- **Use Case**: Business-critical applications with moderate requirements

<Steps>
  <Step title="Create DR Cluster">
    Provision minimal cluster in different region:
    ```yaml
    Cluster: production-dr
    Region: us-west-2 (if primary is us-east-1)
    Nodes: 1 x t3.medium (minimal)
    Auto-scaling: Enabled (1-10 nodes)
    ```
  </Step>

  <Step title="Deploy Applications (Scaled to 0)">
    Deploy all services with 0 replicas:
    ```yaml
    applications:
      - name: api
        replicas: 0
        min_replicas: 0
        max_replicas: 10

      - name: web
        replicas: 0
        min_replicas: 0
        max_replicas: 5
    ```
  </Step>

  <Step title="Set Up Database Replication">
    Configure cross-region read replicas:
    ```bash
    # Primary database in us-east-1
    # Read replica in us-west-2

    # Via Qovery Console
    Database → Settings → Replication
    Enable Cross-Region Replica: us-west-2
    ```
  </Step>

  <Step title="Prepare Failover Procedure">
    Document and test failover:
    ```bash
    # 1. Scale up DR cluster
    qovery cluster update --cluster dr --nodes 5

    # 2. Scale applications
    qovery application scale --application api --replicas 3
    qovery application scale --application web --replicas 2

    # 3. Promote database replica
    qovery database promote-replica --database-id <dr-db-id>

    # 4. Update DNS (automated via Qovery or manual)
    ```
  </Step>
</Steps>

### 3. Warm Standby (Medium Cost, Quick Recovery)

**Strategy**: Scaled-down version always running, ready to scale up

**Implementation with Qovery**:
- Full DR environment running at reduced capacity
- All applications running with minimal replicas
- Databases with read replicas or continuous replication
- Load balancer ready but not active

**Recovery Process**:
1. Scale up applications to full capacity
2. Promote database replicas if needed
3. Switch DNS/load balancer to DR cluster
4. Monitor and adjust

**Characteristics**:
- **RTO**: 5-15 minutes
- **RPO**: 1-5 minutes
- **Cost**: Medium (reduced capacity infrastructure)
- **Complexity**: Medium
- **Use Case**: Production applications requiring quick recovery

<Tabs>
  <Tab title="Configuration">
    **Primary Environment (us-east-1)**:
    ```yaml
    cluster: production
    nodes: 5 x t3.xlarge

    applications:
      api:
        replicas: 5
        cpu: 1000m
        memory: 2GB

      web:
        replicas: 3
        cpu: 500m
        memory: 1GB

    database:
      type: PostgreSQL
      instance: db.r5.xlarge
      multi_az: true
    ```

    **DR Environment (us-west-2)**:
    ```yaml
    cluster: production-dr
    nodes: 2 x t3.large

    applications:
      api:
        replicas: 1  # Minimal capacity
        cpu: 1000m
        memory: 2GB

      web:
        replicas: 1  # Minimal capacity
        cpu: 500m
        memory: 1GB

    database:
      type: PostgreSQL
      instance: db.r5.large  # Smaller instance
      read_replica_of: production-db
    ```
  </Tab>

  <Tab title="Failover Script">
    ```bash
    #!/bin/bash
    # failover-to-dr.sh

    set -e

    echo "Starting failover to DR region..."

    # 1. Scale up DR cluster
    echo "Scaling DR cluster..."
    qovery cluster update \
      --cluster production-dr \
      --nodes 5 \
      --instance-type t3.xlarge

    # 2. Scale applications to production capacity
    echo "Scaling applications..."
    qovery application scale --application api --replicas 5
    qovery application scale --application web --replicas 3

    # 3. Promote database replica to primary
    echo "Promoting database replica..."
    qovery database promote-replica --database-id ${DR_DB_ID}

    # 4. Health check
    echo "Waiting for services to be healthy..."
    qovery environment status --environment production-dr --wait-for-ready

    # 5. Update DNS (example with Route53)
    echo "Updating DNS..."
    aws route53 change-resource-record-sets \
      --hosted-zone-id ${HOSTED_ZONE_ID} \
      --change-batch file://failover-dns.json

    echo "Failover complete! Verify application functionality."
    ```
  </Tab>

  <Tab title="Monitoring">
    Set up alerts to detect when failover is needed:

    ```yaml
    # Alert on primary region failure
    alerts:
      - name: Primary Region Down
        condition: cluster.production.status != "healthy"
        duration: 5m
        actions:
          - notify: ops-team
          - notify: on-call
          - runbook: /docs/dr-failover

      - name: Database Replication Lag
        condition: database.replication_lag > 60s
        duration: 5m
        actions:
          - notify: ops-team

      - name: Application Health Check Failed
        condition: application.health_check.failures > 10
        duration: 2m
        actions:
          - notify: ops-team
    ```
  </Tab>
</Tabs>

### 4. Hot Standby / Multi-Region Active-Active (Highest Cost)

**Strategy**: Full capacity running in multiple regions simultaneously

**Implementation with Qovery**:
- Identical clusters in multiple regions
- Applications running at full capacity in both regions
- Database with multi-region active-active or active-passive replication
- Global load balancing across regions

**Recovery Process**:
1. Automatic failover via global load balancer
2. Monitor remaining region for increased load
3. Scale up if needed
4. Plan recovery of failed region

**Characteristics**:
- **RTO**: < 1 minute (automatic)
- **RPO**: < 1 minute (continuous replication)
- **Cost**: Highest (2x infrastructure)
- **Complexity**: High
- **Use Case**: Mission-critical applications requiring near-zero downtime

<AccordionGroup>
  <Accordion title="Multi-Region Setup">
    **Region 1: us-east-1 (Primary)**:
    ```yaml
    cluster: production-us-east
    nodes: 5 x t3.xlarge
    environments:
      - production-us

    applications:
      - api (5 replicas)
      - web (3 replicas)
      - worker (2 replicas)

    database:
      type: PostgreSQL
      instance: db.r5.xlarge
      multi_az: true
      replication: enabled
    ```

    **Region 2: eu-west-1 (Secondary)**:
    ```yaml
    cluster: production-eu-west
    nodes: 5 x t3.xlarge
    environments:
      - production-eu

    applications:
      - api (5 replicas)
      - web (3 replicas)
      - worker (2 replicas)

    database:
      type: PostgreSQL
      instance: db.r5.xlarge
      multi_az: true
      replication: enabled
    ```

    **Global Load Balancer**:
    ```yaml
    load_balancer:
      type: AWS Global Accelerator / Cloudflare
      health_checks:
        - region: us-east-1
          endpoint: /health
          interval: 10s
        - region: eu-west-1
          endpoint: /health
          interval: 10s

      failover:
        automatic: true
        threshold: 3 failed checks
    ```
  </Accordion>

  <Accordion title="Database Replication">
    **Active-Passive Replication**:
    ```yaml
    primary_database:
      region: us-east-1
      instance: db.r5.xlarge
      multi_az: true

    replica_database:
      region: eu-west-1
      instance: db.r5.xlarge
      replication_lag: < 1s
      read_only: true
      promote_on_failure: automatic
    ```

    **Active-Active Replication** (Advanced):
    ```yaml
    # Using Postgres with logical replication
    database_us:
      region: us-east-1
      bidirectional_replication: enabled
      conflict_resolution: last-write-wins

    database_eu:
      region: eu-west-1
      bidirectional_replication: enabled
      conflict_resolution: last-write-wins
    ```
  </Accordion>

  <Accordion title="Traffic Management">
    **Geographic Routing**:
    ```yaml
    routing:
      - users: north-america
        primary: us-east-1
        failover: eu-west-1

      - users: europe
        primary: eu-west-1
        failover: us-east-1

      - users: asia
        primary: ap-southeast-1
        failover: us-east-1
    ```

    **Health-Based Routing**:
    ```yaml
    health_checks:
      - check: /api/health
        interval: 10s
        timeout: 5s
        healthy_threshold: 2
        unhealthy_threshold: 3

      failover:
        automatic: true
        notify: ops-team
    ```
  </Accordion>
</AccordionGroup>

## Qovery Features for Disaster Recovery

### 1. Environment Cloning

Quickly replicate entire environments for DR testing:

```bash
# Clone production to DR region
qovery environment clone \
  --from production \
  --to production-dr \
  --cluster production-dr \
  --clone-data false
```

### 2. Terraform Export

Export configuration for infrastructure as code:

```bash
# Export environment configuration
qovery environment export \
  --environment production \
  --format terraform \
  --output ./terraform/production/

# Version control
git add terraform/
git commit -m "Backup production state"
git push
```

### 3. Database Backups

Automated backups with point-in-time recovery:

```bash
# List available backups
qovery database backup list --database-id <db-id>

# Restore from backup
qovery database restore \
  --database-id <db-id> \
  --backup-id <backup-id> \
  --target new-database

# Point-in-time recovery
qovery database restore \
  --database-id <db-id> \
  --point-in-time "2024-01-15T10:30:00Z"
```

### 4. Multi-Cluster Management

Manage multiple clusters across regions:

```bash
# List all clusters
qovery cluster list

# Deploy to multiple clusters
qovery environment deploy --environment production --cluster us-east
qovery environment deploy --environment production-dr --cluster us-west
```

### 5. Automated Deployments

Consistent deployments across regions:

```yaml
# .qovery.yml
environments:
  - name: production-primary
    cluster: us-east-1

  - name: production-dr
    cluster: us-west-2

# Deploy to both
git push  # Triggers deployment to both environments
```

## DR Testing Strategy

Regular testing ensures your DR plan works when needed:

### Quarterly DR Drills

<Steps>
  <Step title="Plan the Drill">
    - Schedule during low-traffic period
    - Notify team members
    - Document expected outcomes
    - Prepare rollback plan
  </Step>

  <Step title="Execute Failover">
    ```bash
    # Simulate primary region failure
    # Failover to DR region
    ./failover-to-dr.sh

    # Verify all services
    curl https://api-dr.example.com/health
    ```
  </Step>

  <Step title="Verify Functionality">
    - Test critical user flows
    - Verify database connectivity
    - Check application performance
    - Validate data integrity
  </Step>

  <Step title="Measure Metrics">
    - Record actual RTO
    - Record actual RPO
    - Document issues encountered
    - Calculate cost of running DR
  </Step>

  <Step title="Failback to Primary">
    ```bash
    # Restore primary region
    # Failback from DR
    ./failback-to-primary.sh

    # Verify primary is healthy
    ```
  </Step>

  <Step title="Post-Drill Review">
    - Update DR documentation
    - Fix identified issues
    - Update runbooks
    - Train team on findings
  </Step>
</Steps>

### Continuous Validation

Automated checks to ensure DR readiness:

```yaml
# DR health checks (run daily)
dr_checks:
  - name: Backup Verification
    check: Latest backup < 24 hours old
    alert: ops-team

  - name: DR Cluster Health
    check: DR cluster status = healthy
    alert: ops-team

  - name: Replication Lag
    check: Database replication lag < 60s
    alert: ops-team

  - name: DR Application Deployment
    check: All applications deployed to DR
    alert: ops-team

  - name: DNS Configuration
    check: Failover DNS records configured
    alert: ops-team
```

## Best Practices

<CardGroup cols={2}>
  <Card title="Automate Everything" icon="robot">
    Automate backup, replication, and failover processes to reduce RTO and human error
  </Card>

  <Card title="Test Regularly" icon="vial">
    Conduct quarterly DR drills to validate procedures and identify gaps
  </Card>

  <Card title="Document Procedures" icon="book">
    Maintain detailed runbooks for failover and recovery procedures
  </Card>

  <Card title="Monitor Continuously" icon="chart-line">
    Monitor DR infrastructure health and replication lag constantly
  </Card>

  <Card title="Multi-Region Strategy" icon="globe">
    Deploy across multiple cloud regions to protect against regional failures
  </Card>

  <Card title="Version Control Everything" icon="code-branch">
    Store infrastructure configuration in Git for easy recreation
  </Card>

  <Card title="Separate Credentials" icon="key">
    Use separate credentials for DR to prevent cascade failures
  </Card>

  <Card title="Cost Optimization" icon="dollar-sign">
    Balance DR capability with cost using appropriate strategy for each workload
  </Card>
</CardGroup>

## Cost Comparison

Estimated monthly costs for DR strategies (based on typical production workload):

| Strategy | Infrastructure Cost | RTO | RPO | Best For |
|----------|-------------------|-----|-----|----------|
| **Backup & Restore** | ~$50 (backups only) | 2-4 hours | 24 hours | Development, non-critical apps |
| **Pilot Light** | ~$500 (10% of production) | 30-60 min | 5-15 min | Business-critical apps |
| **Warm Standby** | ~$2,000 (40% of production) | 5-15 min | 1-5 min | Production apps |
| **Hot Standby** | ~$10,000 (200% of production) | < 1 min | < 1 min | Mission-critical apps |

<Info>
Costs vary based on application size, data volume, and cloud provider. Use Qovery's cost optimization features to minimize DR expenses.
</Info>

## Next Steps

<CardGroup cols={2}>
  <Card title="Multi-Cluster Management" href="/guides/use-cases/multi-kubernetes-clusters-management">
    Learn how to manage multiple clusters for DR
  </Card>

  <Card title="Database Backups" href="/using-qovery/configuration/database#backup-and-restore">
    Configure automated database backups
  </Card>

  <Card title="Environment Cloning" href="/using-qovery/configuration/environment#cloning-environments">
    Clone environments for DR setup
  </Card>

  <Card title="Terraform Export" href="/integrations/terraform/overview">
    Export infrastructure as Terraform code
  </Card>
</CardGroup>
